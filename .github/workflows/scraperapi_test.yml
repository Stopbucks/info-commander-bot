##########################################################################
# ğŸ§ª ScraperAPI ä½å®…ä»£ç†ï¼šä¸‰å¤§è·¯å¾‘å£“åŠ›æ¸¬è©¦
##########################################################################
# æ¸¬è©¦ç›®æ¨™ï¼š
# 1. Archive.org (å¤§å‹æª”æ¡ˆé€£ç·šç©©å®šæ€§)
# 2. NASA.gov (æ”¿åºœç¶²åŸŸé˜²ç«ç‰†ç©¿é€)
# 3. LibriVox (æ¨™æº–éç‡Ÿåˆ©ä¼ºæœå™¨)
##########################################################################
##########################################################################
# ğŸ§ª ScraperAPI å–®é»æ”»å …æ¸¬è©¦ v1.3
##########################################################################
# ç›®çš„ï¼šæ¸¬è©¦å–®ä¸€ç©©å®šç¶²å€ï¼Œç¢ºèª HTTP/1.1 èˆ‡æ¨™é ­æ·¨åŒ–æ˜¯å¦æ ¹é™¤ 400 éŒ¯èª¤ã€‚
##########################################################################

name: ğŸ§ª ScraperAPI Single Point Test

on:
  workflow_dispatch: # æ‰‹å‹•å•Ÿå‹•

jobs:
  single_target_test:
    runs-on: ubuntu-latest
    steps:
    - name: ğŸ“¦ æª¢å‡ºç¨‹å¼ç¢¼
      uses: actions/checkout@v4

    - name: ğŸ è¨­å®š Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: âš™ï¸ å®‰è£å¿…è¦ä¾è³´
      run: |
        sudo apt-get update && sudo apt-get install -y ffmpeg
        python -m pip install --upgrade pip
        pip install "curl_cffi>=0.7.0" "requests[socks]" feedparser google-generativeai groq PySocks sgmllib3k python-dotenv google-cloud-storage supabase

    - name: ğŸš€ åŸ·è¡Œ Archive.org ä¸‹è¼‰æ¼”ç¿’
      env:
        SCRAP_API_KEY: ${{ secrets.SCRAP_API_KEY }}
        PYTHONUNBUFFERED: 1
      run: |
        echo "ğŸ’ [å–®é»æ”»å …å•Ÿå‹•] æ­£åœ¨å»ºç«‹å°ˆç”¨æ¸¬è©¦è…³æœ¬..."
        
        # ğŸš€ è…³æœ¬é å·¦å°é½Šï¼Œè§£æ±º IndentationError
        cat <<EOF > scraper_single_test.py
        import os
        import sys
        import time
        # æ³¨å…¥ src æœå°‹è·¯å¾‘
        sys.path.append(os.path.join(os.getcwd(), 'src'))
        from podcast_navigator import NetworkNavigator

        # ğŸ¯ é–å®šå–®ä¸€æœ€ç©©ç›®æ¨™ï¼šInternet Archive
        target = {
            "name": "Archive_Sherlock", 
            "url": "https://archive.org/download/OTRR_Sherlock_Holmes_Sir_Arthur_Conan_Doyle_Library/Sherlock_Holmes_480321_025_The_Case_of_the_Innocent_Murderess.mp3"
        }

        mock_config = {
            "squad_name": "Scraper_Single_Pilot",
            "identity_hash": "smoke_test_001",
            "path_id": "RE", # ğŸš€ å¿…é ˆç‚º RE æ‰èƒ½è§¸ç™¼æ‚¨åœ¨ navigator.py å¯«çš„é™ç´šé‚è¼¯
            "transport_proxy": f"http://scraperapi:{os.environ.get('SCRAP_API_KEY')}@proxy-server.scraperapi.com:8001",
            "curl_config": {"impersonate": "chrome124"}
        }

        print(f"ğŸ› ï¸ [ç’°å¢ƒæº–å‚™] é–‹å§‹å° {target['name']} ç™¼èµ·å–®é»æ¸¬è©¦...")

        try:
            with NetworkNavigator(mock_config) as nav:
                save_path = "test_single_output.mp3"
                
                # 1. åŸ·è¡Œ RE å…æª¢æ”¾è¡Œ
                check = nav.run_pre_flight_check()
                
                # 2. åŸ·è¡Œä¸‹è¼‰ (æ‡‰è§¸ç™¼ HTTP/1.1 èˆ‡ verify=False)
                if check.get("status"):
                    print(f"ğŸ“¡ é‹è¼¸é€šé“å·²é–‹å•Ÿ: {target['url']}")
                    if nav.download_podcast(target['url'], save_path):
                        size = os.path.getsize(save_path) / (1024*1024)
                        print(f"âœ… [æ¸¬è©¦å¤§æ·] æˆåŠŸæŠ“å›éŸ³æª”ï¼š{size:.2f} MB")
                    else:
                        print("âŒ [é‹è¼¸å¤±æ•—] Navigator.download_podcast å›å‚³ Falseã€‚")
                else:
                    print("âŒ [é€£ç·šé˜»å¡] ä»£ç†ä¼ºæœå™¨æ¡æ‰‹å¤±æ•—ã€‚")
        except Exception as e:
            print(f"ğŸ’¥ [ç¨‹å¼å´©æ½°] éŒ¯èª¤åŸå› : {str(e)}")

        print("\nğŸ æ¸¬è©¦ä»»å‹™çµæŸã€‚")
        EOF

        python scraper_single_test.py