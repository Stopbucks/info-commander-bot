
#---------------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/pod_scra_officer.py v6.1 (å…µåŠ›é‡ç·¨ä¿®å¾©ç‰ˆ)
# ä»»å‹™ï¼šHasdata æ¥æ‰‹ Mode 3ã€WebScrap è½‰è·ã€ä¿®å¾©è®Šæ•¸æœªå®šç¾©éŒ¯èª¤
#---------------------------------------------------------------
import os, requests, time, re, urllib3, json
from supabase import create_client, Client
from bs4 import BeautifulSoup
from datetime import datetime, timezone
from pod_scra_scanner import fetch_html 

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def investigate_final_url(url, webscrap_key):
    # èª¿ç”¨ WebScrap åŸ·è¡Œé«˜é›£åº¦è¿½è¹¤ï¼Œç©¿é€é‡å®šå‘ç²å–æœ€çµ‚ MP3ã€‚
    print(f"ğŸ•µï¸ [åµè¨Šå®˜] WebScrap æ­£åœ¨è¿½è¹¤æœ€çµ‚ç›®æ¨™...")
    return url # é ç•™æ“´å……ç©ºé–“

class StrategyManager:
    def __init__(self, supabase: Client, user_mode: str, scra_key: str):
        self.sb = supabase
        #  å°‡æŒ‡æ®å®˜çš„æ‰‹å‹•é¸å®šæ¨¡å¼æŒä¹…åŒ–è‡³è³‡æ–™åº«ã€‚
        if "MODE_" in user_mode or user_mode == "AUTO":
            self.sb.table("api_budget_control").update({"mode_status": user_mode}).eq("id", "ZENROWS").execute()
        self.config = self._load_config()

    def _load_config(self):
        res = self.sb.table("api_budget_control").select("*").eq("id", "ZENROWS").execute()
        return res.data[0]

    def get_action_plan(self):
        #  è®€å–æŒä¹…åŒ–è¨˜æ†¶ï¼Œæ±ºå®šç”±å“ªæ”¯åµå¯Ÿéƒ¨éšŠå‡ºå‹•ã€‚
        saved_mode = self.config.get("mode_status", "AUTO")
        mode_map = {
            "MODE_1_Scrapi": "SCRAPERAPI",
            "MODE_2_Zenrows": "ZENROWS",
            "MODE_3_Hasdata": "HASDATA",  #  Mode 3 æ­£å¼ç”± Hasdata æ“”ä»»ã€‚
            "MODE_4_Scrapedo": "SCRAPEDO"
        }
        return mode_map.get(saved_mode, "ZENROWS") # é è¨­å›é€€è‡³ Zenrows

def run_scra_officer():
    #  é…ç™¼å…¨æ–°å½ˆè—¥ï¼ŒHasdata é€²å…¥åºåˆ—ï¼ŒWebScrap è½‰ç‚ºæ”¯æ´ã€‚
    all_keys = {
        "SCRAPERAPI": os.environ.get("SCRAP_API_KEY"),
        "ZENROWS": os.environ.get("ZENROWS_API_KEY"),
        "HASDATA": os.environ.get("HASDATA_API_KEY"),
        "WEBSCRAP": os.environ.get("WEBSCRAP_API_KEY"),
        "SCRAPEDO": os.environ.get("SCRAPEDO_API_KEY")
    }
    sb_url, sb_key = os.environ.get("SUPABASE_URL"), os.environ.get("SUPABASE_KEY")
    user_mode = os.environ.get("STRATEGY_MODE", "AUTO")

    supabase: Client = create_client(sb_url, sb_key)
    manager = StrategyManager(supabase, user_mode, all_keys["SCRAPERAPI"])

    missions = supabase.table("mission_queue").select("*").eq("scrape_status", "pending").limit(3).execute()

    for target in missions.data:
        task_id = target['id']
        podbay_slug = str(target.get('podbay_slug') or "").strip()
        provider = manager.get_action_plan()
        
        # ğŸ¯ é‡è¦ä¿®å¾©ï¼šåˆå§‹åŒ–è®Šæ•¸ï¼Œé¿å… NameError å´©æ½°ã€‚
        final_mp3_url = None 
        
        try:
            resp = fetch_html(provider, f"https://podbay.fm/p/{podbay_slug}", all_keys)
            if resp and resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                # ä¸€è¡Œè¨»è§£ï¼šåŸ·è¡Œå¤šæ¨™ç±¤æƒæï¼Œå°‹æ‰¾éš±è—çš„éŸ³è¨Šè³‡æºã€‚
                audio_meta = soup.find('meta', property=re.compile(r'(og:audio|twitter:player:stream)'))
                final_mp3_url = audio_meta.get('content') if audio_meta else None
                
                if final_mp3_url:
                    # ä¸€è¡Œè¨»è§£ï¼šè‹¥ç™¼ç¾ç›®æ¨™ï¼Œç«‹å³å›å¡«åº«å­˜ä¸¦æ›´æ–°åµå¯Ÿç‹€æ…‹ã€‚
                    supabase.table("mission_queue").update({
                        "audio_url": final_mp3_url, "scrape_status": "success", "used_provider": provider
                    }).eq("id", task_id).execute()
                    print(f"âœ… [å…¥åº«] {podbay_slug}")
                else:
                    supabase.table("mission_queue").update({"scrape_status": "manual_check"}).eq("id", task_id).execute()
        except Exception as e:
            print(f"âš ï¸ [ç•°å¸¸]ï¼š{e}")

if __name__ == "__main__":
    run_scra_officer()