
#---------------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/pod_scra_officer.py v3.1 (ç²¾æº–ç¯€æµåŠ å›ºç‰ˆ)
# ä»»å‹™ï¼šé™åˆ¶é»æ•¸æ¶ˆè€— (limit 2) -> é›™å‘å¡«å…¥ç¶²å€ -> å¼·åŒ–è§£æ
# æµç¨‹ï¼šé€éscraperAPIã€å»£åŸŸæƒæ MP3 æ¨™ç±¤ã€å®‰å…¨å‚™æ´
#---------------------------------------------------------------
import os, requests, urllib.parse, time, re, urllib3
from supabase import create_client, Client
from bs4 import BeautifulSoup
from datetime import datetime, timezone

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def run_scra_officer():
    # è³‡å®‰å®ˆå‰‡ï¼šåš´æ ¼å¾ç’°å¢ƒè®Šæ•¸è®€å–é‡‘é‘°
    sb_url = os.environ.get("SUPABASE_URL")
    sb_key = os.environ.get("SUPABASE_KEY")
    scra_key = os.environ.get("SCRAP_API_KEY")
    
    if not all([sb_url, sb_key, scra_key]):
        print("âŒ [è³‡å®‰è­¦å ±] ç¼ºå°‘å¿…è¦ç’°å¢ƒè®Šæ•¸ï¼Œçµ‚æ­¢è¡Œå‹•ã€‚")
        return

    supabase: Client = create_client(sb_url, sb_key)

    # ğŸš€ æ¥µé™ç¯€æµï¼šæ¯æ¬¡åƒ…æå– 1 ç­†å¾…è™•ç†ä»»å‹™ï¼Œç¢ºä¿å‰©é¤˜ 145 é»èƒ½æ”¯æ’æœ€å¾Œ 5 æ¬¡æ ¸å¿ƒæ¸¬è©¦
    missions = supabase.table("mission_queue").select("*").eq("scrape_status", "pending").limit(1).execute()
    
    if not missions.data: 
        print(f"â˜• [{datetime.now().strftime('%H:%M:%S')}] å¾…å‘½ï¼šç›®å‰ç„¡å¾…è™•ç†ä»»å‹™ã€‚")
        return

    for target in missions.data:
        task_id = target['id']
        raw_title = target['episode_title']
        podbay_slug = target.get('podbay_slug') or "bloomberg-businessweek"
        final_mp3_url = ""

        print(f"ğŸ¯ [ç²¾æº–ç‹™æ“Š] ç›®æ¨™é›†æ•¸ï¼š{raw_title[:30]}...")

        try:
            # æˆ°å ´ä¸€ï¼šPodbay è¼•é‡åµå¯Ÿ (ä¸æ¸²æŸ“ï¼Œçœé»æ•¸)
            program_home = f"https://podbay.fm/p/{podbay_slug}"
            home_url = f"https://api.scraperapi.com?api_key={scra_key}&url={urllib.parse.quote(program_home)}"
            resp = requests.get(home_url, timeout=30, verify=False)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            ep_tag = soup.find('a', href=re.compile(r'/p/.+/e/.*'))
            if ep_tag:
                full_ep_url = f"https://podbay.fm{ep_tag['href']}"
                
                # æˆ°å ´äºŒï¼šæ·±åº¦è§£ç¢¼ (é–‹å•Ÿæ¸²æŸ“ï¼Œé è¨ˆæ¶ˆè€—ç´„ 22 é»)
                ep_encoded = urllib.parse.quote(full_ep_url)
                print(f"ğŸ” [åŸ·è¡Œæ¸²æŸ“] æ­£åœ¨æå– MP3 ç›´é€£é–€ç¥¨...")
                ep_res = requests.get(f"https://api.scraperapi.com?api_key={scra_key}&url={ep_encoded}&render=true", timeout=60)
                ep_soup = BeautifulSoup(ep_res.text, 'html.parser')
                
                # å»£åŸŸæƒæéŸ³è¨Šæ¨™ç±¤
                audio_meta = ep_soup.find('meta', property=re.compile(r'(og:audio|twitter:player:stream)'))
                if audio_meta:
                    final_mp3_url = audio_meta.get('content')
                else:
                    mp3_link = ep_soup.find('a', href=re.compile(r'\.mp3'))
                    if mp3_link: final_mp3_url = mp3_link['href']
        except Exception as e:
            print(f"âš ï¸ [åµå¯Ÿç•°å¸¸]ï¼š{str(e)}")

        # --- æœ€çµ‚çµç®— ---
        if final_mp3_url:
            # ğŸ’¡ åŠ å›ºï¼šåŒæ™‚æ›´æ–° audio_url èˆ‡ podbay_urlï¼Œå¾¹åº•è§£æ±ºé‹è¼¸å…µæŠ“ä¸åˆ°è³‡æ–™çš„å•é¡Œ
            update_data = {
                "podbay_url": final_mp3_url,
                "audio_url": final_mp3_url,
                "scrape_status": "success",
                "status": "pending", 
                "created_at": datetime.now(timezone.utc).isoformat() 
            }
            supabase.table("mission_queue").update(update_data).eq("id", task_id).execute()
            print(f"âœ… [å…¥åº«æˆåŠŸ] é–€ç¥¨ç™¼æ”¾ï¼š{final_mp3_url[:60]}...")
        else:
            # è‹¥å¤±æ•—å‰‡æ¨™è¨˜ï¼Œé¿å…é‡è¤‡æµªè²»é»æ•¸
            supabase.table("mission_queue").update({
                "scrape_status": "failed",
                "status": "failed"
            }).eq("id", task_id).execute()
            print(f"âŒ [ä»»å‹™å¤±æ•—] ç„¡æ³•ç²å– MP3 é€£çµã€‚")

if __name__ == "__main__":
    run_scra_officer()