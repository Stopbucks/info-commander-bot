
#---------------------------------------------------------------
#---------------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/pod_scra_officer.py v5.0 (S-Plan æ¨¡çµ„åŒ–è§£è€¦ç‰ˆ)
# ä»»å‹™ï¼šæˆ°ç•¥èª¿åº¦ã€æ¨¡å¼æŒä¹…åŒ–ã€æœˆåˆè‡ªå‹•æ ¡æº–ã€å‘¼å«å¤–éƒ¨ Scanner
#---------------------------------------------------------------
import os, requests, urllib.parse, time, re, urllib3, random
from supabase import create_client, Client
from bs4 import BeautifulSoup
from datetime import datetime, timezone
from pod_scra_scanner import fetch_html # ğŸš€ å°å…¥å¤–éƒ¨æƒæå™¨

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def get_scraperapi_balance(api_key):
    """å³æ™‚é™æ¸¬ï¼šç²å– ScraperAPI å‰©é¤˜é»æ•¸"""
    try:
        res = requests.get(f"https://api.scraperapi.com/account?api_key={api_key}", timeout=10)
        if res.status_code == 200:
            data = res.json()
            return data.get('requests_available', 78)
    except Exception as e:
        print(f"âš ï¸ [é™æ¸¬å¤±æ•—]: {e}")
    return 78

class StrategyManager:
    """æˆ°ç•¥ç®¡ç†å™¨ï¼šè² è²¬å¤šè»Œåˆ‡æ›èˆ‡é»æ•¸æŒä¹…åŒ–æ§åˆ¶"""
    def __init__(self, supabase: Client, user_mode: str, scra_key: str):
        self.sb = supabase
        self.scra_key = scra_key
        # è‹¥æœ‰æ‰‹å‹•è¼¸å…¥æ¨¡å¼ï¼Œå„ªå…ˆæŒä¹…åŒ–å¯«å…¥ DB è¨˜æ†¶
        if "MODE_" in user_mode or user_mode == "AUTO":
            self.sb.table("api_budget_control").update({"mode_status": user_mode}).eq("id", "ZENROWS").execute()
            print(f"ğŸ’¾ [æˆ°ç•¥å­˜æª”] æ¨¡å¼è¨­å®šé–å®šç‚ºï¼š{user_mode}")
        self.config = self._load_config()

    def _load_config(self):
        """æœˆåˆè‡ªå‹•æ ¡æº–èˆ‡è¼‰å…¥è¨­å®š"""
        res = self.sb.table("api_budget_control").select("*").eq("id", "ZENROWS").execute()
        data = res.data[0]
        if datetime.now().day == 1 and data['last_reset_date'] != str(datetime.now().date()):
            update_fields = {"balance": 1000, "mode_status": "AUTO", "last_reset_date": str(datetime.now().date())}
            self.sb.table("api_budget_control").update(update_fields).eq("id", "ZENROWS").execute()
            data.update(update_fields)
            print("ğŸ“… [æœˆåˆæ ¡æº–] é»æ•¸å·²é‡ç½®ï¼Œæˆ°ç•¥å›æ­¸ AUTO æ¨¡å¼ã€‚")
        return data

    def get_action_plan(self):
        """æ ¸å¿ƒæ±ºç­–é‚è¼¯ï¼šæ ¹æ“š DB è¨˜æ†¶æˆ–å³æ™‚é»æ•¸æ±ºå®šä¾›æ‡‰å•†"""
        saved_mode = self.config.get("mode_status", "AUTO")
        
        # æ˜ å°„è¡¨ï¼šå°‡ YAML é¸å–®æ¨™ç±¤è½‰ç‚º Scanner è­˜åˆ¥ç¢¼
        mode_map = {
            "MODE_1_Scrapi": "SCRAPERAPI",
            "MODE_2_Zenrows": "ZENROWS",
            "MODE_3_Webscrap": "WEBSCRAPING",
            "MODE_4_Scrapedo": "SCRAPEDO"
        }

        # 1. è™•ç†æ‰‹å‹•æŒ‡å®šçš„å›ºå®šæ¨¡å¼
        if saved_mode in mode_map:
            print(f"ğŸ•¹ï¸ [æ‰‹å‹•æ¨¡å¼] æŒ‡æ®å®˜æŒ‡ä»¤ï¼šæ¡ç”¨ {mode_map[saved_mode]}")
            return mode_map[saved_mode]

        # 2. è™•ç† AUTO è‡ªå‹•æ¨¡å¼é‚è¼¯ (3/3 å‰å„ªå…ˆä½¿ç”¨ Zenrows è©¦ç”¨)
        scra_balance = get_scraperapi_balance(self.scra_key)
        print(f"ğŸ“Š ScraperAPI å³æ™‚åº«å­˜ï¼š{scra_balance} é»")
        if scra_balance > 80:
            return "SCRAPERAPI"
        else:
            print(f"ğŸš¨ [è‡ªå‹•é¿éšª] ScraperAPI ä¸è¶³ï¼Œåˆ‡æ›è‡³ä¸»åŠ›å‚™æ´ ZENROWS")
            return "ZENROWS"

    def deduct_points(self, provider):
        """æ ¹æ“šä¸åŒä¾›æ‡‰å•†æ‰£é™¤ DB ä¸­çš„é ä¼°é»æ•¸ (çµ±ä¸€æš«ä¼°æ¯æ¬¡ 25 é»)"""
        # åƒ… ZENROWS ç›®å‰æœ‰åœ¨ DB ç´€éŒ„ balanceï¼Œå…¶é¤˜ provider æš«ç‚ºç´€éŒ„æ€§è³ª
        if provider == "ZENROWS":
            new_balance = max(0, self.config['balance'] - 25)
            self.sb.table("api_budget_control").update({"balance": new_balance}).eq("id", "ZENROWS").execute()
            print(f"ğŸ“‰ [æ‰£é»] {provider} é ä¼°é¤˜é¡ï¼š{new_balance}")

def run_scra_officer():
    # 1. è®€å–æˆ°ç•¥é‡‘é‘°
    all_keys = {
        "SCRAPERAPI": os.environ.get("SCRAP_API_KEY"),
        "ZENROWS": os.environ.get("ZENROWS_API_KEY"),
        "WEBSCRAP": os.environ.get("WEBSCRAP_API_KEY"),
        "SCRAPEDO": os.environ.get("SCRAPEDO_API_KEY")
    }
    sb_url = os.environ.get("SUPABASE_URL")
    sb_key = os.environ.get("SUPABASE_KEY")
    user_mode = os.environ.get("STRATEGY_MODE", "AUTO")

    if not all([sb_url, sb_key, all_keys["SCRAPERAPI"], all_keys["ZENROWS"]]):
        print("âŒ [è³‡å®‰è­¦å ±] ç¼ºå°‘é—œéµç’°å¢ƒè®Šæ•¸ã€‚")
        return

    supabase: Client = create_client(sb_url, sb_key)
    manager = StrategyManager(supabase, user_mode, all_keys["SCRAPERAPI"])

    # æˆ°è¡“ä¼‘çœ ï¼šæ¨¡æ“¬çœŸäººè¡Œç‚º
    start_delay = random.randint(600, 2400)
    print(f"ğŸ•’ [æˆ°è¡“ç­‰å¾…] éš¨æ©Ÿä¼‘çœ  {start_delay//60} åˆ†é˜...")
    time.sleep(start_delay)

    # 2. é ˜å–åµå¯Ÿä»»å‹™
    missions = supabase.table("mission_queue").select("*") \
        .eq("scrape_status", "pending") \
        .limit(3).execute()

    for target in missions.data:
        task_id = target['id']
        podbay_slug = str(target.get('podbay_slug') or "").strip()

        if not podbay_slug or podbay_slug.isdigit():
            print(f"âš ï¸ [æ•¸æ“šç•°å¸¸] Slug {podbay_slug} ç„¡æ•ˆï¼Œæ¨™è¨˜æ‰‹å‹•æª¢æŸ¥ã€‚")
            supabase.table("mission_queue").update({"scrape_status": "manual_check"}).eq("id", task_id).execute()
            continue

        # 3. ç²å–ä½œæˆ°è¨ˆç•«ä¸¦å‘¼å«æƒæå™¨
        provider = manager.get_action_plan()
        target_page = f"https://podbay.fm/p/{podbay_slug}"
        print(f"ğŸ¯ [åŸ·è¡Œä¸­] ä¾›æ‡‰å•†ï¼š{provider} | ç›®æ¨™ï¼š{podbay_slug}")

        try:
            # ğŸš€ è§£è€¦æ ¸å¿ƒï¼šä¸€è¡Œèª¿ç”¨å¤–éƒ¨æƒæå™¨ï¼Œä¸å†ç®¡åƒæ•¸ç´°ç¯€
            resp = fetch_html(provider, target_page, all_keys)
            manager.deduct_points(provider)

            if resp and resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                audio_meta = soup.find('meta', property=re.compile(r'(og:audio|twitter:player:stream)'))
                final_mp3_url = audio_meta.get('content') if audio_meta else None
                
                if not final_mp3_url:
                    mp3_link = soup.find('a', href=re.compile(r'\.mp3'))
                    final_mp3_url = mp3_link['href'] if mp3_link else None

                if final_mp3_url:
                    # 4. ç‰©è³‡å…¥åº«
                    update_data = {
                        "audio_url": final_mp3_url,
                        "scrape_status": "success",
                        "used_provider": provider, 
                        "status": "pending",
                        "created_at": datetime.now(timezone.utc).isoformat()
                    }
                    supabase.table("mission_queue").update(update_data).eq("id", task_id).execute()
                    print(f"âœ… [æˆåŠŸ] ç‰©è³‡å·²å…¥åº«ï¼Œæ¨™ç±¤ç‚ºï¼š{provider}")
                else:
                    print(f"ğŸ” [æœªç™¼ç¾éŸ³æª”] ç¶²é è§£ææˆåŠŸä½†ç„¡ MP3ï¼Œæ¨™è¨˜æ‰‹å‹•æª¢æŸ¥ã€‚")
                    supabase.table("mission_queue").update({"scrape_status": "manual_check"}).eq("id", task_id).execute()
            else:
                print(f"âŒ [è«‹æ±‚å¤±æ•—] ä¾›æ‡‰å•†å›å ±ç‹€æ…‹ç¢¼ï¼š{resp.status_code if resp else 'No Resp'}")

        except Exception as e:
            print(f"âš ï¸ [ç¨‹åºç•°å¸¸]ï¼š{str(e)}")

if __name__ == "__main__":
    run_scra_officer()