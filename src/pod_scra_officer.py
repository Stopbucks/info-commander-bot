
#---------------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/pod_scra_officer.py v3.1 (ç²¾æº–ç¯€æµåŠ å›ºç‰ˆ)
# ä»»å‹™ï¼šé™åˆ¶é»æ•¸æ¶ˆè€— (limit 2) -> é›™å‘å¡«å…¥ç¶²å€ -> å¼·åŒ–è§£æ
# æµç¨‹ï¼šé€éscraperAPIã€å»£åŸŸæƒæ MP3 æ¨™ç±¤ã€å®‰å…¨å‚™æ´
#---------------------------------------------------------------

import os, requests, urllib.parse, time, re, urllib3
from supabase import create_client, Client
from bs4 import BeautifulSoup
from datetime import datetime, timezone

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def run_scra_officer():
    sb_url = os.environ.get("SUPABASE_URL")
    sb_key = os.environ.get("SUPABASE_KEY")
    scra_key = os.environ.get("SCRAP_API_KEY")
    supabase: Client = create_client(sb_url, sb_key)

    # ğŸš€ å½ˆè—¥ç®¡åˆ¶ï¼šæ¯æ¬¡åƒ…æå– 2 ç­†å¾…è™•ç†ä»»å‹™ï¼Œåš´æ ¼æ§åˆ¶ ScraperAPI é»æ•¸æ¶ˆè€—
    missions = supabase.table("mission_queue").select("*").eq("scrape_status", "pending").limit(2).execute()
    if not missions.data: 
        print("â˜• [å¾…å‘½] æš«ç„¡å¾…è™•ç†ä»»å‹™ã€‚")
        return

    for target in missions.data:
        task_id = target['id']
        raw_title = target['episode_title']
        podbay_slug = target.get('podbay_slug') or "bloomberg-businessweek"
        final_mp3_url = ""

        print(f"ğŸ“¡ [åµå¯Ÿé–‹å§‹]ï¼š{raw_title[:20]}...")

        # --- æˆ°å ´ä¸€ï¼šPodbay è¼•é‡åµå¯Ÿ ---
        try:
            program_home = f"https://podbay.fm/p/{podbay_slug}"
            # ğŸš€ ç¯€æµå„ªåŒ–ï¼šä¸é–‹å•Ÿ render=true åƒ…æ¶ˆè€— 1 é»
            home_url = f"https://api.scraperapi.com?api_key={scra_key}&url={urllib.parse.quote(program_home)}"
            resp = requests.get(home_url, timeout=30, verify=False)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # å°‹æ‰¾é›†æ•¸é€£çµ
            ep_tag = soup.find('a', href=re.compile(r'/p/.+/e/.*'))
            if ep_tag:
                full_ep_url = f"https://podbay.fm{ep_tag['href']}"
                
                # ğŸš€ ç²¾æº–æ‰“æ“Šï¼šåƒ…åœ¨ç¢ºå®šæœ‰é›†æ•¸é æ™‚æ‰ä½¿ç”¨æ¸²æŸ“(æ¶ˆè€— 5 é»ä»¥ä¸Š) [cite: 2026-02-16]
                ep_encoded = urllib.parse.quote(full_ep_url)
                ep_res = requests.get(f"https://api.scraperapi.com?api_key={scra_key}&url={ep_encoded}&render=true", timeout=60)
                ep_soup = BeautifulSoup(ep_res.text, 'html.parser')
                
                # å»£åŸŸæƒæéŸ³è¨Šæ¨™ç±¤
                audio_meta = ep_soup.find('meta', property=re.compile(r'(og:audio|twitter:player:stream)'))
                if audio_meta:
                    final_mp3_url = audio_meta.get('content')
                else:
                    # å‚™æ´ï¼šæœå°‹ .mp3 é€£çµ
                    mp3_link = ep_soup.find('a', href=re.compile(r'\.mp3'))
                    if mp3_link: final_mp3_url = mp3_link['href']
        except Exception as e:
            print(f"âš ï¸ [è§£æç•°å¸¸]ï¼š{str(e)}")

        # --- æœ€çµ‚çµç®— (æ¥åŠ›è½‰å‹) ---
        if final_mp3_url:
            # ğŸš€ ä¿®æ­£é‡é»ï¼šåŒæ™‚å›å¡« podbay_url èˆ‡ audio_urlï¼Œç¢ºä¿é‹è¼¸å…µä¸æœƒæŠ“ç©º
            update_data = {
                "podbay_url": final_mp3_url,
                "audio_url": final_mp3_url, # ç¢ºä¿é‹è¼¸å…µæŠ“å–æ­¤æ¬„ä½
                "scrape_status": "success",
                "status": "pending", 
                "created_at": datetime.now(timezone.utc).isoformat() 
            }
            supabase.table("mission_queue").update(update_data).eq("id", task_id).execute()
            print(f"âœ… [æˆåŠŸå…¥åº«] é–€ç¥¨ç™¼æ”¾ï¼š{final_mp3_url[:50]}...")
        else:
            supabase.table("mission_queue").update({
                "scrape_status": "failed",
                "status": "failed"
            }).eq("id", task_id).execute()
            print(f"âŒ [å¤±æ•—] ç„¡æ³•å–å¾—é€£çµã€‚")

if __name__ == "__main__":
    run_scra_officer()