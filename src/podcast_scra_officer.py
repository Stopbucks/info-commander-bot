#---------------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ç‚ºï¼špodcast_scra_officer.py 
# å¾ mission_queue é ˜å‘½(scrape_status = 'pending')ï¼Œä»¥ScraperAPI (8001 ç«¯å£) 
# å‰å¾€ Podbay ç²¾ç¢ºå®šä½é›†æ•¸ï¼Œæå– MP3 é–€ç¥¨ç¶²å€ï¼Œå¸¶å›ç¶²å€å¯«å…¥ podbay_url ä¸¦æ¨™è¨˜ç‚º successã€‚
#---------------------------------------------------------------

import os, requests, urllib.parse, time, re
from supabase import create_client, Client
from bs4 import BeautifulSoup

def clean_title(title):
    # ğŸš€ æˆ°è¡“æ´—æ»Œ v2.4ï¼šè™•ç†å™ªéŸ³ã€æ‹¬è™Ÿèˆ‡éé•·å­—ä¸²
    # 1. ç§»é™¤å¸¸è¦‹å™ªéŸ³å‰ç¶´ (å¦‚ Replay -, Update -) [cite: 2026-02-16]
    title = re.sub(r'^(Replay|Update|Special)\s*[-:]\s*', '', title, flags=re.IGNORECASE)
    # 2. ç§»é™¤æ‹¬è™Ÿå…§å®¹ (å¦‚ (æº«é¤Šæ—¥))
    title = re.sub(r'\(.*?\)', '', title)
    # 3. ç§»é™¤å†’è™Ÿèˆ‡ç ´æŠ˜è™Ÿå¾Œé¢çš„å…§å®¹ (é€šå¸¸æ˜¯å­æ¨™é¡Œï¼Œæœƒå¹²æ“¾æœå°‹)
    title = title.split(' - ')[0].split(': ')[0]
    # 4. åªå–å‰ 5 å€‹å–®å­—ï¼Œå¢åŠ æœå°‹å¯¬å®¹åº¦ [cite: 2026-02-16]
    words = title.split()
    return " ".join(words[:5]).strip()

def run_scra_officer():
    sb_url = os.environ.get("SUPABASE_URL")
    sb_key = os.environ.get("SUPABASE_KEY")
    scra_key = os.environ.get("SCRAP_API_KEY")

    if not all([sb_url, sb_key, scra_key]):
        print("âŒ [æ†‘è­‰éºå¤±]")
        return

    supabase: Client = create_client(sb_url, sb_key)
    proxy_url = f"http://scraperapi:{scra_key}@proxy-server.scraperapi.com:8001"
    proxies = {"http": proxy_url, "https": proxy_url}

    # ğŸš€ æ¨¡æ“¬è‡ªå‹•åŒ–ï¼šé ˜å– 3 ç­†å¾…è™•ç†ä»»å‹™ (åŒ…å«æ‚¨å‰›æ‰åœ¨ Supabase æ‰‹å‹•é‡ç½®çš„ç­†æ•¸)
    missions = supabase.table("mission_queue").select("*").eq("scrape_status", "pending").limit(3).execute()

    if not missions.data:
        print("â˜• [åº«å­˜æ¸…ç©º] æ²’æœ‰ pending ä»»å‹™ã€‚")
        return

    for target in missions.data:
        task_id = target['id']
        raw_title = target['episode_title']
        search_query = clean_title(raw_title)
        
        print(f"\nğŸ“¡ [æ¸¬è©¦ä»»å‹™] åŸå§‹ï¼š{raw_title[:30]}...")
        print(f"ğŸ” [æ´—æ»Œé—œéµå­—]ï¼š{search_query}")

        try:
            encoded_query = urllib.parse.quote(search_query)
            podbay_search = f"https://podbay.fm/search?q={encoded_query}"
            
            # ä½¿ç”¨ ScraperAPI æ”»å …
            #resp = requests.get(podbay_search, proxies=proxies, timeout=40, verify=False)
            #å…©å€‹ç¶²ç«™æ”»å …ï¼Œæ‹‰é•·æ™‚é–“
            resp = requests.get(target_url, proxies=proxies, timeout=60, verify=False)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # æœå°‹åŒ¹é…é›†æ•¸é€£çµ
            ep_link_tag = soup.find('a', href=re.compile(r'/p/.+/e/.*'))
            
            if ep_link_tag:
                full_ep_url = f"https://podbay.fm{ep_link_tag['href']}"
                print(f"ğŸ¯ [å®šä½æˆåŠŸ] ç¶²å€ï¼š{full_ep_url}")

                ep_resp = requests.get(full_ep_url, proxies=proxies, timeout=30, verify=False)
                ep_soup = BeautifulSoup(ep_resp.text, 'html.parser')
                
                audio_tag = ep_soup.find('meta', property="og:audio")
                final_mp3_url = audio_tag['content'] if audio_tag else ""

                if final_mp3_url:
                    supabase.table("mission_queue").update({
                        "podbay_url": final_mp3_url,
                        "scrape_status": "success"
                    }).eq("id", task_id).execute()
                    print(f"âœ… [å…¥åº«æˆåŠŸ] MP3 å·²å°±ç·’ã€‚")
                else:
                    print("âŒ [é–€ç¥¨éºå¤±] é é¢å…§æ‰¾ä¸åˆ° MP3ã€‚")
            else:
                print(f"âš ï¸ [æœå°‹å¤±æ•—] Podbay æ‰¾ä¸åˆ°ï¼š{search_query}")
                supabase.table("mission_queue").update({"scrape_status": "failed"}).eq("id", task_id).execute()

        except Exception as e:
            print(f"ğŸ’¥ [æ•…éšœ] {str(e)}")
        
        time.sleep(2) # æˆ°è¡“å–˜æ¯

if __name__ == "__main__":
    run_scra_officer()