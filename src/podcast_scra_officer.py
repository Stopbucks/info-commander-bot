#---------------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ç‚ºï¼špodcast_scra_officer.py 
# ç‰ˆæœ¬ï¼šv2.6 é›™æˆ°å ´å¯¦æˆ°ç‰ˆ (Podbay + Listen Notes)
#---------------------------------------------------------------

import os, requests, urllib.parse, time, re, urllib3
from supabase import create_client, Client
from bs4 import BeautifulSoup

# ğŸš€ é—œé–‰å®‰å…¨è­¦å‘Šï¼Œè®“æ—¥èªŒæ›´ä¹¾æ·¨
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def clean_search_query(source_name, episode_title):
    # æˆ°è¡“æ´—æ»Œï¼šåˆä½µç¯€ç›®åç¨±èˆ‡å‰ 5 å€‹å–®å­—ï¼Œæé«˜ç²¾ç¢ºåº¦ [cite: 2026-02-16]
    source_clean = re.sub(r'\(.*?\)', '', source_name).strip()
    ep_words = episode_title.split()
    ep_clean = " ".join(ep_words[:5])
    return re.sub(r'[^\w\s]', ' ', f"{source_clean} {ep_clean}").strip()

def run_scra_officer():
    sb_url = os.environ.get("SUPABASE_URL")
    sb_key = os.environ.get("SUPABASE_KEY")
    scra_key = os.environ.get("SCRAP_API_KEY")

    if not all([sb_url, sb_key, scra_key]):
        print("âŒ [æ†‘è­‰éºå¤±]")
        return

    supabase: Client = create_client(sb_url, sb_key)
    proxy_url = f"http://scraperapi:{scra_key}@proxy-server.scraperapi.com:8001"
    proxies = {"http": proxy_url, "https": proxy_url}

    # é ˜å– 3 ç­†å¾…è™•ç†ä»»å‹™
    missions = supabase.table("mission_queue").select("*").eq("scrape_status", "pending").limit(3).execute()

    if not missions.data:
        print("â˜• [åº«å­˜æ¸…ç©º] æ²’æœ‰ pending ä»»å‹™ã€‚")
        return

    for target in missions.data:
        task_id = target['id']
        raw_title = target['episode_title']
        source_name = target.get('source_name', '')
        search_query = clean_search_query(source_name, raw_title)
        final_mp3_url = ""

        print(f"\nğŸ“¡ [ä»»å‹™å•Ÿå‹•] ç¯€ç›®ï¼š{source_name} | é—œéµå­—ï¼š{search_query}")

        # --- æˆ°å ´ä¸€ï¼šPodbay æ”»å … ---
        try:
            encoded_query = urllib.parse.quote(search_query)
            podbay_url = f"https://podbay.fm/search?q={encoded_query}"
            
            # ğŸ’¡ å·²ä¿®æ­£ï¼šä½¿ç”¨ podbay_url ä¸¦å°‡è¶…æ™‚å»¶è‡³ 60 ç§’ [cite: 2026-02-16]
            resp = requests.get(podbay_url, proxies=proxies, timeout=60, verify=False)
            soup = BeautifulSoup(resp.text, 'html.parser')
            ep_link_tag = soup.find('a', href=re.compile(r'/p/.+/e/.*'))
            
            if ep_link_tag:
                full_ep_url = f"https://podbay.fm{ep_link_tag['href']}"
                print(f"ğŸ¯ [Podbay ç™¼ç¾]ï¼š{full_ep_url}")
                ep_resp = requests.get(full_ep_url, proxies=proxies, timeout=30, verify=False)
                audio_tag = BeautifulSoup(ep_resp.text, 'html.parser').find('meta', property="og:audio")
                final_mp3_url = audio_tag['content'] if audio_tag else ""
        except Exception as e:
            print(f"âš ï¸ Podbay æ•…éšœï¼š{str(e)[:50]}")

        # --- æˆ°å ´äºŒï¼šListen Notes å‚™æ´ (è‹¥ Podbay æ²’æŠ“åˆ°) --- [cite: 2026-02-16]
        if not final_mp3_url:
            print(f"ğŸ”„ [å•Ÿå‹•å‚™æ´] è½‰å‘ Listen Notes æ”»å …...")
            try:
                ln_search = f"https://www.listennotes.com/search/?q={encoded_query}&scope=episode"
                resp = requests.get(ln_search, proxies=proxies, timeout=60, verify=False)
                soup = BeautifulSoup(resp.text, 'html.parser')
                # Listen Notes çš„é›†æ•¸é€£çµç‰¹å¾µ [cite: 2026-02-16]
                ln_link = soup.find('a', href=re.compile(r'/podcasts/.+/.+'))
                
                if ln_link:
                    ln_url = f"https://www.listennotes.com{ln_link['href']}"
                    print(f"ğŸ¯ [LN å®šä½æˆåŠŸ]ï¼š{ln_url}")
                    ln_resp = requests.get(ln_url, proxies=proxies, timeout=30, verify=False)
                    audio_tag = BeautifulSoup(ln_resp.text, 'html.parser').find('meta', property="og:audio")
                    final_mp3_url = audio_tag['content'] if audio_tag else ""
            except Exception as e:
                print(f"âš ï¸ Listen Notes æ•…éšœï¼š{str(e)[:50]}")

        # --- å›å¡«çµæœ ---
        if final_mp3_url:
            supabase.table("mission_queue").update({
                "podbay_url": final_mp3_url,
                "scrape_status": "success"
            }).eq("id", task_id).execute()
            print(f"âœ… [å…¥åº«æˆåŠŸ] MP3 ç¶²å€å·²å¸¶å›ã€‚")
        else:
            print(f"âŒ [å…¨é¢å¤±å®ˆ] Podbay èˆ‡ LN å‡ç„¡æ³•è§£ç¢¼ï¼š{search_query}")
            supabase.table("mission_queue").update({"scrape_status": "failed"}).eq("id", task_id).execute()

        time.sleep(3) # æˆ°è¡“ä¼‘æ¯

if __name__ == "__main__":
    run_scra_officer()