# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/test_scanner_expedition.py v2.5 (æ·±åº¦è§£æç‰ˆ)
# ä»»å‹™ï¼šåˆ©ç”¨ Regex æš´åŠ›æª¢ç´¢æŠ€è¡“ï¼Œå¾ Scrapedo/ZenRows å¸¶å›çš„ä»£ç¢¼æµä¸­æŒ–æ˜ MP3
# ---------------------------------------------------------
import os, time, random, re, urllib3
from supabase import create_client, Client
from bs4 import BeautifulSoup
from datetime import datetime, timezone
from pod_scra_scanner import fetch_html 

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- [å€å¡Šä¸‰ï¼šæ·±åº¦æƒ…å ±æŒ–æ˜ (Deep Recon)] ---
def extract_audio_url_v25(html_content):
    """
    ä¸€è¡Œè¨»è§£ï¼šä¸å†ä¾è³´ Meta æ¨™ç±¤ï¼Œç›´æ¥é‡å°å…¨ç¶²é ä»£ç¢¼é€²è¡Œ .mp3 ç‰¹å¾µæå–ã€‚
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 1. å‚³çµ± Meta æƒæ
    audio_meta = soup.find('meta', property=re.compile(r'(og:audio|twitter:player:stream)'))
    if audio_meta and audio_meta.get('content'): return audio_meta.get('content')
    
    # 2. æ­£å‰‡è¡¨é”å¼ã€Œæ·±æµ·æœç´¢ã€ ğŸš€
    # ä¸€è¡Œè¨»è§£ï¼šæœå°‹ä»»ä½•åŒ…å« http...mp3 çš„å­—ä¸²ï¼Œé€™æ˜¯å°ä»˜ SPA ç¶²é çš„æœ€å¼·æ­¦å™¨ã€‚
    mp3_pattern = r'https?://[^\s"\'<>]+?\.mp3[^\s"\'<>]*'
    found_links = re.findall(mp3_pattern, html_content)
    
    if found_links:
        # éæ¿¾æ‰æ˜é¡¯ç„¡æ•ˆçš„é€£çµ (å¦‚å¸¶æœ‰ query string çš„é‡è¤‡é …)
        valid_link = found_links[0]
        print(f"ğŸ”¦ [æ·±æµ·æœç´¢] æˆåŠŸæŒ–æ˜éš±è—ç¶²å€ï¼š{valid_link[:50]}...")
        return valid_link
    
    return None

# --- [ä¸»æ¼”ç¿’ç¨‹åº] ---
def run_expedition_test():

    # 1. å–å¾—æ¼”ç¿’æŒ‡ä»¤
    test_mode = os.environ.get("TEST_PROVIDER_MODE", "ZENROWS")
    target_site = os.environ.get("TEST_SITE_TARGET", "PODBAY")
    
    sb_url = os.environ.get("SUPABASE_URL")
    sb_key = os.environ.get("SUPABASE_KEY")
    all_keys = {
        "SCRAPERAPI": os.environ.get("SCRAP_API_KEY"),
        "ZENROWS": os.environ.get("ZENROWS_API_KEY"),
        "WEBSCRAP": os.environ.get("WEBSCRAP_API_KEY"),
        "SCRAPEDO": os.environ.get("SCRAPEDO_API_KEY"),
        "HASDATA": os.environ.get("HASDATA_API_KEY")
    }

    # ğŸš€ é—œéµä¿®æ­£ç·šï¼šé‡æ–°å»ºç«‹èˆ‡è³‡æ–™åº«çš„é€šè¨Šéˆè·¯
    supabase: Client = create_client(sb_url, sb_key)

    # é ˜å– 3 ç­†å¾…è™•ç†ä»»å‹™
    res = supabase.table("mission_queue").select("*").eq("scrape_status", "pending").limit(3).execute()
    
    if not res.data:
        print("â˜• [å¾…å‘½] ç„¡æ¼”ç¿’ç›®æ¨™ã€‚")
        return

    for index, task in enumerate(res.data):
        if index > 0: time.sleep(random.randint(10, 15))
        
        # æ§‹é€ ç¶²å€ (Podbay é‚è¼¯)
        slug = task.get('podbay_slug')
        target_url = f"https://podbay.fm/p/{slug}"

        print(f"ğŸ¯ [åµå¯Ÿä¸­] ç›®æ¨™ï¼š{slug} | æ¨¡å¼ï¼š{os.environ.get('TEST_PROVIDER_MODE')}")

        try:
            resp = fetch_html(os.environ.get('TEST_PROVIDER_MODE'), target_url, all_keys)

            if resp and resp.status_code == 200:
                # ğŸš€ èª¿ç”¨é€²åŒ–å¾Œçš„è§£ææ¨¡çµ„
                final_mp3_url = extract_audio_url_v25(resp.text)
                
                if final_mp3_url:
                    supabase.table("mission_queue").update({
                        "audio_url": final_mp3_url,
                        "scrape_status": "success",
                        "used_provider": f"{os.environ.get('TEST_PROVIDER_MODE')}_V25"
                    }).eq("id", task['id']).execute()
                    print(f"âœ… [æˆåŠŸ] æƒ…å ±æå–æˆåŠŸï¼")
                else:
                    # ä¸€è¡Œè¨»è§£ï¼šå³ä¾¿å¤±æ•—ä¹Ÿå°å‡ºå‰ 500 å­—å…ƒä¾›åˆ†æã€‚
                    print(f"ğŸ” [ç¼ºå¤±] ä»£ç¢¼é•·åº¦ {len(resp.text)}ï¼Œä½†ç„¡ MP3 ç‰¹å¾µã€‚")
            else:
                print(f"âŒ [å¤±æ•—] ç‹€æ…‹ç¢¼ï¼š{resp.status_code if resp else 'No Resp'}")
        except Exception as e:
            print(f"âš ï¸ [ç•°å¸¸] {e}")

if __name__ == "__main__":
    run_expedition_test()