# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/test_scanner_expedition.py v2.0 (å…¨ç¶­åº¦æ¨¡çµ„åŒ–ç‰ˆ)
# ä»»å‹™ï¼šæ¸¬è©¦å„ä¾›æ‡‰å•†ï¼ˆZENROWS, WEBSCRAPING...ï¼‰å°å„å¤§ç«™é»çš„ç©¿é€èˆ‡æ´»æ€§åŒ–èƒ½åŠ›ã€‚
# ---------------------------------------------------------
import os, time, random, re, urllib3
from supabase import create_client, Client
from bs4 import BeautifulSoup
from datetime import datetime, timezone
from pod_scra_scanner import fetch_html 

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- [å€å¡Šä¸€ï¼šåˆ†é¡åµæ¸¬èˆ‡æ™ºèƒ½èª¿åº¦] ---
def get_scra_response(target_url, test_mode, all_keys):
    """
    ä¸€è¡Œè¨»è§£ï¼šæ ¹æ“šç¶²å€ç‰¹å¾µåˆ¤æ–·æ˜¯å¦å€¼å¾—åŸ·è¡Œ Scrapingï¼Œä¸¦èª¿ç”¨æƒæå™¨ã€‚
    """
    direct_hosts = ["megaphone.fm", "omny.fm", "blubrry.com", "acast.com", "buzzsprout.com", ".mp3"]
    is_direct = any(host in target_url for host in direct_hosts)

    if is_direct and test_mode == "WEBSCRAPING":
        print(f"âš ï¸ [æˆæœ¬è­¦å ±] ç›®æ¨™ç‚ºç›´é€£é€£çµï¼Œæ¸¬è©¦ WebScraping ç©¿é€åŠ›...")
    
    # å‘¼å«çµ±ä¸€æƒæå™¨é‚è¼¯
    return fetch_html(test_mode, target_url, all_keys)

# --- [å€å¡ŠäºŒï¼šç¶²å€æ§‹é€ æ¨¡çµ„] ---
def build_target_url(task, target_site):
    """
    ä¸€è¡Œè¨»è§£ï¼šæ ¹æ“š YML æŒ‡ä»¤ï¼ˆPODBAY/LISTEN_NOTES/OFFICIALï¼‰å»ºæ§‹åµå¯Ÿåº§æ¨™ã€‚
    """
    if target_site == "LISTEN_NOTES":
        slug = task.get('listen_notes_id')
        return f"https://www.listennotes.com/podcasts/{slug}/" if slug else None
    elif target_site == "OFFICIAL":
        # å®˜æ–¹æ¨¡å¼ï¼šæ‹¿èµ· Vercel ç™¼ç¾çš„åŸå§‹ URL ä½œç‚ºæ´»æ€§åŒ–ç¨®å­
        return task.get('audio_url')
    else: # é è¨­ PODBAY
        slug = task.get('podbay_slug')
        return f"https://podbay.fm/p/{slug}" if slug else None

# --- [å€å¡Šä¸‰ï¼šæƒ…å ±æŒ–æ˜æ¨¡çµ„] ---
def extract_audio_url(html_content):
    """
    ä¸€è¡Œè¨»è§£ï¼šå¾ HTML ä¸­æŒ–æ˜å…·æ™‚æ•ˆæ€§çš„ MP3 ç¶²å€ã€‚
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    # å„ªå…ˆæœå°‹ meta æ¨™æº–æ¨™ç±¤ (og:audio æˆ– twitter æ’­æ”¾å™¨)
    audio_meta = soup.find('meta', property=re.compile(r'(og:audio|twitter:player:stream)'))
    if audio_meta:
        return audio_meta.get('content')
    
    # å‚™æ´ï¼šæœå°‹ HTML5 audio æ¨™ç±¤
    audio_tag = soup.find('audio')
    if audio_tag:
        return audio_tag.get('src')
    
    return None

# --- [ä¸»ç¨‹åºï¼šæ¼”ç¿’æ ¸å¿ƒ] ---
def run_expedition_test():
    # 1. å–å¾—æ¼”ç¿’æŒ‡ä»¤
    test_mode = os.environ.get("TEST_PROVIDER_MODE", "ZENROWS")
    target_site = os.environ.get("TEST_SITE_TARGET", "PODBAY")
    
    sb_url = os.environ.get("SUPABASE_URL")
    sb_key = os.environ.get("SUPABASE_KEY")
    all_keys = {
        "SCRAPERAPI": os.environ.get("SCRAP_API_KEY"),
        "ZENROWS": os.environ.get("ZENROWS_API_KEY"),
        "WEBSCRAP": os.environ.get("WEBSCRAP_API_KEY"),
        "SCRAPEDO": os.environ.get("SCRAPEDO_API_KEY"),
        "HASDATA": os.environ.get("HASDATA_API_KEY")
    }

    supabase: Client = create_client(sb_url, sb_key)
    # é ˜å– 3 ç­†å¾…è™•ç†ä»»å‹™
    res = supabase.table("mission_queue").select("*").eq("scrape_status", "pending").limit(3).execute()
    
    if not res.data:
        print("â˜• [å¾…å‘½] å€‰åº«æš«ç„¡ç©å£“ç‰©è³‡ã€‚")
        return

    print(f"ğŸš€ [æ¼”ç¿’é–‹å§‹] æ¨¡å¼ï¼š{test_mode} | ç«™é»ï¼š{target_site} | è¦æ¨¡ï¼š{len(res.data)} ç­†")

    for index, task in enumerate(res.data):
        if index > 0: time.sleep(random.randint(10, 20)) # æ¨¡æ“¬çœŸäºº Jitter
        
        # A. å»ºæ§‹ç›®æ¨™ URL
        target_url = build_target_url(task, target_site)
        if not target_url:
            print(f"â© [è·³é] ä»»å‹™ ID {task['id']} ç¼ºå°‘ç«™é» {target_site} æ‰€éœ€çš„ ID/Slugã€‚")
            continue

        print(f"ğŸ¯ [åµå¯Ÿä¸­] ç›®æ¨™ï¼š{target_url[:60]}...")

        try:
            # B. åŸ·è¡ŒæŠ“å–
            resp = get_scra_response(target_url, test_mode, all_keys)

            if resp and resp.status_code == 404:
                print(f"ğŸš¨ [å°èˆªéŒ¯èª¤] 404 å¤±æ•ˆã€‚è«‹æ ¡å° Supabase å°æ¨™è³‡æ–™ã€‚")
                supabase.table("mission_queue").update({"scrape_status": "manual_check"}).eq("id", task['id']).execute()
                continue

            if resp and resp.status_code == 200:
                # C. æŒ–æ˜æ´»æ€§åŒ–ç¶²å€
                final_mp3_url = extract_audio_url(resp.text)
                
                if final_mp3_url:
                    try:
                        # D. å…¥åº«ä¸¦å¯¦æ–½ 23505 éœé»˜å¿½ç•¥ç­–ç•¥
                        supabase.table("mission_queue").update({
                            "audio_url": final_mp3_url,
                            "scrape_status": "success",
                            "used_provider": f"{test_mode}_TEST_{target_site}"
                        }).eq("id", task['id']).execute()
                        print(f"âœ… [å ±æ·] {target_site} æ´»æ€§åŒ–æˆåŠŸï¼")
                    except Exception as db_e:
                        if "23505" in str(db_e):
                            print(f"â™»ï¸ [é‡è¤‡åµæ¸¬] ç¶²å€å·²åœ¨åº«å­˜ä¸­ï¼Œä»»å‹™æ¨™è¨˜ç‚º successã€‚")
                            supabase.table("mission_queue").update({"scrape_status": "success"}).eq("id", task['id']).execute()
                else:
                    print(f"ğŸ” [æƒ…å ±ç¼ºå¤±] ç¶²é è§£ææˆåŠŸä½†ç„¡ MP3ã€‚å…§å®¹é•·åº¦ï¼š{len(resp.text)}")
            else:
                print(f"âŒ [å¤±æ•—] {test_mode} å›å ±ï¼š{resp.status_code if resp else 'No Resp'}")

        except Exception as e:
            print(f"âš ï¸ [ç•°å¸¸] {test_mode} åŸ·è¡Œå´©æ½°: {e}")

if __name__ == "__main__":
    run_expedition_test()