# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/test_chunked_transport.py v2.0 (å…¨ç¶­åº¦åµå¯Ÿæ¬é‹ç‰ˆ)
# ä»»å‹™ï¼šHEAD åµå¯Ÿã€å‹•æ…‹åˆ†æ®µã€æ“¬æ…‹ç·©è¡æ¬é‹ã€äºŒé€²ä½ç¸«åˆèˆ‡ Opus è½‰ç¢¼
# ---------------------------------------------------------
import os, requests, time, random, boto3, math, subprocess
from supabase import create_client, Client
from datetime import datetime, timezone

# --- [å€å¡Šä¸€ï¼šç‰©è³‡åµå¯Ÿæ¨¡çµ„ (Reconnaissance)] ---
def get_target_specs(url):
    """
    ä¸€è¡Œè¨»è§£ï¼šé€é HEAD è«‹æ±‚é å…ˆç²å–æª”æ¡ˆè¦æ ¼ï¼ˆå¤§å°ã€é¡å‹ï¼‰ï¼Œä½œç‚ºæ¬é‹ç­–ç•¥ä¾æ“šã€‚
    """
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebkit/537.36'}
    try:
        # ä¸€è¡Œè¨»è§£ï¼šallow_redirects=True ç¢ºä¿èƒ½æŠ“åˆ° Megaphone ç­‰è·³è½‰å¾Œçš„æœ€çµ‚æª”æ¡ˆå¤§å°ã€‚
        r = requests.head(url, headers=headers, timeout=15, allow_redirects=True)
        size = int(r.headers.get('Content-Length', 0))
        return size
    except Exception as e:
        print(f"âš ï¸ [é æŸ¥å¤±æ•—] ç„¡æ³•ç²å–ç‰©è³‡é«”ç©ï¼š{e}")
        return 0

# --- [å€å¡ŠäºŒï¼šç‰©æµä¸­ç¹¼æ¨¡çµ„ (Relay)] ---
def fetch_chunk_via_proxy(target_url, start, end, api_key):
    """
    ä¸€è¡Œè¨»è§£ï¼šåˆ©ç”¨ WebScraping.ai çš„ keep_headers åƒæ•¸ï¼Œé€å‚³ Range æ¨™é ­é€²è¡Œåˆ†æ®µæŠ“å–ã€‚
    """
    proxy_params = {
        'api_key': api_key,
        'url': target_url,
        'keep_headers': 'true', # ğŸš€ é—œéµï¼šå¿…é ˆä¿ç•™ Range æ¨™é ­ï¼Œä¼ºæœå™¨æ‰æœƒå›å‚³ 206
        'proxy': 'residential'  # å»ºè­°æ¬é‹æ™‚ä½¿ç”¨ä½å®…ä»£ç†ä»¥é™ä½ 403 é¢¨éšª
    }
    custom_headers = {
        'Range': f'bytes={start}-{end}',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebkit/537.36'
    }
    try:
        resp = requests.get('https://api.webscraping.ai/html', 
                            params=proxy_params, headers=custom_headers, timeout=60)
        return resp.content if resp.status_code in [200, 206] else None
    except Exception:
        return None

# --- [å€å¡Šä¸‰ï¼šç¸«åˆèˆ‡é‡ç·¨æ¨¡çµ„ (Assembler)] ---
def assemble_and_compress(task_id, chunk_count, final_name):
    """
    ä¸€è¡Œè¨»è§£ï¼šå°‡åˆ†æ®µäºŒé€²ä½æª”æ¡ˆæŒ‰åºç¸«åˆï¼Œä¸¦èª¿ç”¨ FFmpeg åŸ·è¡Œ 16K/Mono/Opus é«˜æ•ˆå£“ç¸®ã€‚
    """
    temp_raw = f"{task_id}_merged.mp3"
    # ä¸€è¡Œè¨»è§£ï¼šäºŒé€²ä½ç„¡æç¸«åˆã€‚
    with open(temp_raw, 'wb') as outfile:
        for i in range(chunk_count):
            part_path = f"parts/part_{i}.bin"
            if os.path.exists(part_path):
                with open(part_path, 'rb') as infile: outfile.write(infile.read())
                os.remove(part_path)

    # ä¸€è¡Œè¨»è§£ï¼šFFmpeg è½‰ç¢¼ï¼Œ-b:a 24k ç¢ºä¿ 7MB ä»¥ä¸‹ç›®æ¨™ã€‚
    subprocess.run([
        'ffmpeg', '-y', '-i', temp_raw,
        '-ar', '16000', '-ac', '1', '-c:a', 'libopus', '-b:a', '24k',
        final_name
    ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    
    if os.path.exists(temp_raw): os.remove(temp_raw)
    return os.path.getsize(final_name)

# --- [ä¸»æ¼”ç¿’ç¨‹åº (Main Expedition)] ---
def run_relay_expedition():
    # 1. è£œçµ¦èˆ‡åˆå§‹åŒ–
    scra_key = os.environ.get("WEBSCRAP_API_KEY")
    r2_id, r2_secret = os.environ.get("R2_ACCESS_KEY_ID"), os.environ.get("R2_SECRET_ACCESS_KEY")
    r2_acc, r2_bucket = os.environ.get("R2_ACCOUNT_ID"), os.environ.get("R2_BUCKET_NAME")
    
    s3_client = boto3.client('s3', endpoint_url=f'https://{r2_acc}.r2.cloudflarestorage.com',
                             aws_access_key_id=r2_id, aws_secret_access_key=r2_secret)

    # ğŸš€ æ¨¡æ“¬å¾ Supabase é ˜å–ä¸€å€‹å¯¦éš›ä»»å‹™ (æ­¤è™•ç¤ºç¯„ç¶²å€)
    target_url = "https://traffic.megaphone.fm/WSJ2187157396.mp3"
    task_id = "TASK_" + datetime.now().strftime('%m%d%H%M')

    # ğŸš€ 2. å‰ç½®åµå¯Ÿ (æ±ºå®šæ¬é‹ç­–ç•¥)
    total_size_bytes = get_target_specs(target_url)
    if total_size_bytes == 0:
        print("ğŸ›‘ [çµ‚æ­¢] ç„¡æ³•é æŸ¥ç‰©è³‡è¦æ¨¡ï¼Œæ”¾æ£„å‡ºæ“Šã€‚")
        return
    
    total_mb = total_size_bytes / (1024 * 1024)
    print(f"ğŸ“Š [æƒ…å ±å ±æ·] ç‰©è³‡é«”ç©ï¼š{total_mb:.2f} MB")

    # ğŸš€ æˆ°ç•¥åˆ†æµï¼šè‹¥æª”æ¡ˆå¤ªå°ï¼ˆ<1.2MBï¼‰å‰‡å–®æ¬¡æ¬é‹ï¼Œä¸åˆ†æ®µã€‚
    if total_mb < 1.2:
        print("ğŸ’¡ [ç­–ç•¥å„ªåŒ–] ç‰©è³‡æ¥µè¼•ï¼Œåˆ‡æ›è‡³å–®æ¬¡æ¬é‹æ¨¡å¼...")
        chunk_size = total_size_bytes
    else:
        chunk_size = 1024 * 1024 # 1MB æ¨™æº–ç‰‡æ®µ

    num_chunks = math.ceil(total_size_bytes / chunk_size)
    if not os.path.exists('parts'): os.makedirs('parts')

    print(f"ğŸš€ [æ¼”ç¿’é–‹å§‹] æ¨¡å¼ï¼šæ“¬æ…‹ç·©è¡åˆ†æ®µ | ç‰‡æ®µæ•¸ï¼š{num_chunks} | é è¨ˆæ¶ˆè€—ï¼š{num_chunks} é»")

    # 3. åŸ·è¡Œåºåˆ—åŒ–æ¬é‹ (æ“¬äººåŒ–è™•ç†)
    for i in range(num_chunks):
        start = i * chunk_size
        end = min(start + chunk_size - 1, total_size_bytes - 1)
        
        # ä¸€è¡Œè¨»è§£ï¼šé¦–ç‰‡æ®µå³æ™‚æŠ“å–ï¼Œå¾ŒçºŒç‰‡æ®µæ¨¡æ“¬äººé¡ã€Œæ’­æ”¾ç·©è¡ã€æŠ–å‹•ã€‚
        if i > 0:
            jitter = random.uniform(4.5, 9.2) 
            print(f"ğŸ•’ [æ“¬æ…‹ç·©è¡] ç­‰å¾… {jitter:.2f} ç§’ä»¥é¿é–‹åµæ¸¬...")
            time.sleep(jitter)

        chunk_data = fetch_chunk_via_proxy(target_url, start, end, scra_key)
        
        if chunk_data:
            with open(f"parts/part_{i}.bin", "wb") as f: f.write(chunk_data)
            print(f"âœ… ç‰‡æ®µ {i} æ¬é‹å®Œæˆã€‚")
        else:
            print(f"âŒ [é‡å¤§æç›Š] ç‰‡æ®µ {i} éºå¤±ï¼Œæœ¬æ¬¡æ¼”ç¿’å®£å‘Šå¤±æ•—ã€‚")
            return

    # 4. ç¸«åˆã€å£“ç¸®èˆ‡å¾Œç½®æ ¡é©—
    final_opus = f"RELAY_{task_id}.opus"
    compressed_size = assemble_and_compress(task_id, num_chunks, final_opus)
    
    # ä¸€è¡Œè¨»è§£ï¼šè¨ˆç®—æœ€çµ‚æˆ°æœï¼Œè©•ä¼°å£“ç¸®æ¯”ç‡ã€‚
    ratio = (compressed_size / total_size_bytes) * 100
    print(f"ğŸ“ˆ [å¾Œç½®æ ¡é©—] åŸå§‹ï¼š{total_mb:.2f}MB -> å£“ç¸®å¾Œï¼š{compressed_size/(1024*1024):.2f}MB (æ•ˆç‡ï¼š{ratio:.1f}%)")

    # 5. ç‰©è³‡å…¥åº«
    s3_client.upload_file(final_opus, r2_bucket, final_opus, ExtraArgs={'ContentType': 'audio/ogg'})
    print(f"ğŸ† [æ¼”ç¿’æˆåŠŸ] ç‰©è³‡å·²å…¥åº«ï¼š{final_opus}")

if __name__ == "__main__":
    run_relay_expedition()