# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/test_chunked_transport.py v4.5 (ç´”ä»£ç†æ”»å …ç‰ˆ)
# ä»»å‹™ï¼š60MB é–€æª»ã€4.5MB åˆ†æ®µã€ç´”ä»£ç†äºŒé€²ä½é€å‚³ã€FFmpeg ç¸«åˆã€AI å ±æˆ°
# ---------------------------------------------------------
import os, requests, time, random, boto3, math, subprocess, urllib3
from supabase import create_client, Client
from datetime import datetime
from podcast_ai_agent import AIAgent 

# ä¸€è¡Œè¨»è§£ï¼šç¦ç”¨ä»£ç†æ¨¡å¼ç”¢ç”Ÿçš„ SSL å®‰å…¨æ†‘è­‰è­¦å‘Šï¼Œç¢ºä¿æ—¥èªŒæ•´æ½”ã€‚
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- [å€å¡Šä¸€ï¼šç‰©è³‡è¦æ ¼èˆ‡ç›´é€£åµå¯Ÿ] ---
def get_target_specs(url):
    """ä¸€è¡Œè¨»è§£ï¼šåŸ·è¡Œæœ¬åœ° HEAD è«‹æ±‚ä»¥ç²å–æœ€çµ‚ç›´é€£ä½å€èˆ‡æª”æ¡ˆé«”ç©ã€‚"""
    stealth_headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }
    try:
        r = requests.head(url, headers=stealth_headers, timeout=15, allow_redirects=True)
        return int(r.headers.get('Content-Length', 0)), r.url
    except Exception as e:
        print(f"âš ï¸ [åµå¯Ÿå—é˜»] {e}")
        return 0, url

# --- [å€å¡ŠäºŒï¼šç´”ä»£ç†ç‰©æµä¸­ç¹¼æ¨¡çµ„ v4.5] ---
def fetch_chunk_via_pure_proxy(target_url, start, end, api_key):
    """ä¸€è¡Œè¨»è§£ï¼šé€é WebScraping.ai 8888 ç«¯å£åŸ·è¡Œç´”ä»£ç†å‚³è¼¸ï¼Œç¢ºä¿äºŒé€²ä½æµä¸è¢« HTML æ±¡æŸ“ã€‚"""
    # ä¸€è¡Œè¨»è§£ï¼šå°‡æ§åˆ¶åƒæ•¸å°è£ç‚ºå¯†ç¢¼ï¼Œjs=false èˆ‡ residential ç¢ºä¿é«˜ç©¿é€åŠ›ã€‚
    proxy_params = "js=false&proxy=residential"
    # ä¸€è¡Œè¨»è§£ï¼šå»ºæ§‹èªè­‰ä»£ç† URLï¼Œæ¡ç”¨ Basic Auth æ ¼å¼ã€‚
    proxy_url = f"http://{api_key}:{proxy_params}@proxy.webscraping.ai:8888"
    
    proxies = {"http": proxy_url, "https": proxy_url}
    headers = {
        'Range': f'bytes={start}-{end}',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://anchor.fm/'
    }

    try:
        # ä¸€è¡Œè¨»è§£ï¼šä½¿ç”¨ verify=False ä»¥ç›¸å®¹ä»£ç†å•†è‡ªç°½åæ†‘è­‰ã€‚
        resp = requests.get(target_url, headers=headers, proxies=proxies, timeout=60, verify=False)
        
        if resp.status_code == 206:
            # ä¸€è¡Œè¨»è§£ï¼šåŸ·è¡Œå“è³ªæŒ‡ç´‹æª¢é©—ï¼Œè‹¥å…§å®¹å¤ªå°æˆ–å« HTML æ¨™ç±¤å‰‡ç†”æ–·ã€‚
            if b"<html" in resp.content[:100].lower():
                print(f"âš ï¸ [æ””æˆªè­¦å ±] ä»£ç†å›å‚³äº† HTML æ®¼å±¤è€ŒéäºŒé€²ä½ç¢ç‰‡ã€‚")
                return None
            return resp.content
        print(f"âŒ [ç‹€æ…‹ç•°å¸¸] éŸ¿æ‡‰ç¢¼ï¼š{resp.status_code}")
        return None
    except Exception as e:
        print(f"âš ï¸ [é€£ç·šå´©æ½°] {e}")
        return None

# --- [å€å¡Šä¸‰ï¼šFFmpeg ç¸«åˆèˆ‡é‡ç·¨æ¨¡çµ„] ---
def assemble_and_compress(task_id, chunk_count, final_name, source_url):
    """ä¸€è¡Œè¨»è§£ï¼šåˆä½µç¢ç‰‡ä¸¦åŸ·è¡Œ 16K/Mono/Opus å£“ç¸®ï¼Œå„ªåŒ– M4A/MP3 ç´¢å¼•çµæ§‹ã€‚"""
    ext = ".m4a" if ".m4a" in source_url.lower() else ".mp3"
    temp_raw = f"{task_id}_raw{ext}"
    
    with open(temp_raw, 'wb') as outfile:
        for i in range(chunk_count):
            part_path = f"parts/part_{i}.bin"
            if os.path.exists(part_path):
                with open(part_path, 'rb') as infile: outfile.write(infile.read())
                os.remove(part_path)

    # ä¸€è¡Œè¨»è§£ï¼šåŠ å…¥ -movflags faststartï¼Œç¢ºä¿éŸ³è¨Šåœ¨ R2 é è¦½æ™‚èƒ½å³æ™‚æ’­æ”¾ã€‚
    subprocess.run([
        'ffmpeg', '-y', '-err_detect', 'ignore_err', '-i', temp_raw,
        '-ar', '16000', '-ac', '1', '-c:a', 'libopus', '-b:a', '24k',
        '-movflags', 'faststart', final_name
    ], capture_output=True)
    
    if os.path.exists(temp_raw): os.remove(temp_raw)
    return os.path.getsize(final_name)

# --- [ä¸»æ¼”ç¿’ç¨‹åº] ---
def run_full_cycle_test():
    # 1. åˆå§‹åŒ–è£œçµ¦ç·š
    scra_key = os.environ.get("WEBSCRAP_API_KEY")
    supabase = create_client(os.environ.get("SUPABASE_URL"), os.environ.get("SUPABASE_KEY"))
    ai_agent = AIAgent()
    s3_client = boto3.client('s3', endpoint_url=f"https://{os.environ.get('R2_ACCOUNT_ID')}.r2.cloudflarestorage.com",
                             aws_access_key_id=os.environ.get('R2_ACCESS_KEY_ID'), 
                             aws_secret_access_key=os.environ.get('R2_SECRET_ACCESS_KEY'))

    # ğŸš€ 2. é ˜å–å¾…å‘½ç‰©è³‡
    res = supabase.table("mission_queue").select("*").eq("status", "pending") \
        .not_.is_("audio_url", "null").order("created_at", desc=True).limit(1).execute()

    if not res.data: return print("â˜• [å¾…å‘½] ç„¡æ¼”ç¿’ç‰©è³‡ã€‚")
    m = res.data[0]
    
    # ğŸš€ 3. å…ˆè¡Œè§£æç›´é€£ä½å€ (é—œéµæ–¥å€™è¡Œå‹•)
    total_size, resolved_url = get_target_specs(m['audio_url'])
    total_mb = total_size / (1024 * 1024)
    
    if total_size == 0 or total_mb > 60:
        return print(f"ğŸ›‘ [æ’¤é€€] é«”ç© ({total_mb:.2f}MB) è¶…æ¨™æˆ–ç„¡æ•ˆã€‚")

    # ğŸš€ 4. åˆ†æ®µæ±ºç­– (æ¡ç”¨ 4.5MB ç©©å¥è¼‰è·)
    chunk_size = max(4.5 * 1024 * 1024, math.ceil(total_size / 15))
    num_chunks = math.ceil(total_size / chunk_size)
    if not os.path.exists('parts'): os.makedirs('parts')

    print(f"ğŸš€ [æ¼”ç¿’å•Ÿå‹•] {m['source_name']} | ç¸½é‡ï¼š{total_mb:.2f}MB | åˆ†æ®µï¼š{num_chunks}")

    # ğŸš€ 5. åºåˆ—åŒ–ä»£ç†æ¬é‹
    for i in range(num_chunks):
        if i > 0: time.sleep(random.uniform(7.5, 12.5))
        start, end = i * chunk_size, min((i + 1) * chunk_size - 1, total_size - 1)

        # ä¸€è¡Œè¨»è§£ï¼šç™¼å‹• v4.5 ç´”ä»£ç†æ¨¡å¼æ¬é‹ã€‚
        chunk_data = fetch_chunk_via_pure_proxy(resolved_url, start, end, scra_key)
        
        if chunk_data:
            with open(f"parts/part_{i}.bin", "wb") as f: f.write(chunk_data)
            print(f"âœ… ç‰‡æ®µ {i} æˆåŠŸã€‚")
        else:
            return print(f"âŒ [ä¸­æ–·] ç‰‡æ®µ {i} ç²å–å¤±æ•—ï¼ŒåŸ·è¡Œç†”æ–·ã€‚")

    # ğŸš€ 6. ç¸«åˆåˆ†æèˆ‡å ±æˆ°
    final_opus = f"RELAY_V45_{datetime.now().strftime('%Y%m%d')}_{m['source_name']}.opus"
    c_size = assemble_and_compress(m['id'], num_chunks, final_opus, resolved_url)
    analysis, _, duration = ai_agent.generate_gold_analysis(final_opus)

    if analysis:
        report = ai_agent.format_mission_report("Proxy-V4.5", m['episode_title'], resolved_url, analysis, datetime.now().strftime("%m/%d/%y"), duration, m['source_name'])
        report += f"\n\nğŸ“Š [æ•¸æ“š]\nåŸå§‹ï¼š{total_mb:.2f}MB\nå£“ç¸®ï¼š{c_size/(1024*1024):.2f}MB"
        requests.post(f"https://api.telegram.org/bot{os.environ.get('TELEGRAM_BOT_TOKEN')}/sendMessage", 
                      json={"chat_id": os.environ.get("TELEGRAM_CHAT_ID"), "text": report, "parse_mode": "Markdown"})

    # ğŸš€ 7. å…¥åº«æ­¸æª”
    s3_client.upload_file(final_opus, os.environ.get('R2_BUCKET_NAME'), final_opus, ExtraArgs={'ContentType': 'audio/ogg'})
    supabase.table("mission_queue").update({"status": "completed", "r2_url": final_opus}).eq("id", m['id']).execute()
    print(f"ğŸ† [æ¼”ç¿’é”æˆ] ä»»å‹™çµæ¡ˆã€‚")
    if os.path.exists(final_opus): os.remove(final_opus)

if __name__ == "__main__":
    run_full_cycle_test()