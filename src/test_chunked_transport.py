# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/test_chunked_transport.py v3.4 (ç›´é€£è§£æå¼·åŒ–ç‰ˆ)
# ä»»å‹™ï¼š60MB é–€æª»ã€5.5MB å‹•æ…‹åˆ†æ®µã€å…ˆè¡Œè§£æç›´é€£ã€FFmpeg å®¹éŒ¯è½‰ç¢¼ã€AI å ±æˆ°
# ---------------------------------------------------------
import os, requests, time, random, boto3, math, subprocess
from supabase import create_client, Client
from datetime import datetime, timezone
from podcast_ai_agent import AIAgent 

# --- [å€å¡Šä¸€ï¼šç‰©è³‡è¦æ ¼èˆ‡ç›´é€£åµå¯Ÿ (HEAD Recon)] ---
def get_target_specs(url):
    """ä¸€è¡Œè¨»è§£ï¼šé æŸ¥æª”æ¡ˆå¤§å°ä¸¦è¿½è¹¤æœ€çµ‚é‡æ–°å°å‘ç¶²å€ï¼Œç¢ºä¿åˆ†æ®µä¸‹è¼‰æ¨™é ­ä¸éºå¤±ã€‚"""
    stealth_headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    try:
        # ğŸš€ é—œéµï¼šallow_redirects=True ç²å–ç¶“éå¤šæ¬¡è½‰å€å¾Œçš„æœ€çµ‚å¯¦é«”æª”æ¡ˆä½å€
        r = requests.head(url, headers=stealth_headers, timeout=15, allow_redirects=True)
        total_size = int(r.headers.get('Content-Length', 0))
        resolved_url = r.url
        print(f"ğŸ“¡ [å°èˆªè§£æ] æœ€çµ‚ç›´é€£ä½å€ï¼š{resolved_url[:60]}...")
        return total_size, resolved_url
    except Exception as e:
        print(f"âš ï¸ [é æŸ¥å¤±æ•—] ç„¡æ³•ç²å–å¤§å°æˆ–è§£æä½å€ï¼š{e}")
        return 0, url

# --- [å€å¡ŠäºŒï¼šå¼·åŒ–ç‰ˆç‰©æµä¸­ç¹¼æ¨¡çµ„] ---
def fetch_chunk_via_proxy(target_url, start, end, api_key):
    """ä¸€è¡Œè¨»è§£ï¼šé€é WebScraping.ai ä½å®…ä»£ç†æŠ“å–äºŒé€²ä½ç¢ç‰‡ï¼Œä¸¦åŸ·è¡Œ HTML æ±¡æŸ“æª¢æ ¸ã€‚"""
    proxy_params = {
        'api_key': api_key, 'url': target_url, 
        'keep_headers': 'true', 'proxy': 'residential', 'timeout': 30000
    }
    custom_headers = {
        'Range': f'bytes={start}-{end}',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebkit/537.36'
    }
    try:
        resp = requests.get('https://api.webscraping.ai/html', params=proxy_params, headers=custom_headers, timeout=60)
        if resp.status_code in [200, 206]:
            if b"<html" in resp.content[:100].lower():
                print(f"âš ï¸ [æ””æˆªè­¦å ±] ä»£ç†å›å‚³ HTML è€ŒééŸ³è¨Šï¼Œåº§æ¨™ï¼š{start}-{end}")
                return None
            return resp.content
        return None
    except Exception: return None

# --- [å€å¡Šä¸‰ï¼šå¼·åŒ–ç‰ˆç¸«åˆèˆ‡å£“ç¸®] ---
def assemble_and_compress(task_id, chunk_count, final_name, source_url):
    """ä¸€è¡Œè¨»è§£ï¼šæ ¹æ“šç›´é€£ç¶²å€å‹•æ…‹æ±ºå®šå‰¯æª”åï¼Œä¸¦åŸ·è¡Œå…·å‚™ Faststart ç‰¹æ€§çš„ Opus å£“ç¸®ã€‚"""
    ext = ".mp3"
    if ".m4a" in source_url.lower(): ext = ".m4a"
    elif ".wav" in source_url.lower(): ext = ".wav"
    
    temp_raw = f"{task_id}_raw{ext}"
    with open(temp_raw, 'wb') as outfile:
        for i in range(chunk_count):
            part_path = f"parts/part_{i}.bin"
            if os.path.exists(part_path):
                with open(part_path, 'rb') as infile: outfile.write(infile.read())
                os.remove(part_path)

    print(f"ğŸ—œï¸ [å£“ç¸®ä¸­] åŸå§‹æ ¼å¼ {ext}ï¼ŒåŸ·è¡Œ 16K/Mono/Opus è½‰ç¢¼...")
    # ä¸€è¡Œè¨»è§£ï¼šåŠ å…¥ ignore_err èˆ‡ faststartï¼Œç¢ºä¿ä¸²æµæ’­æ”¾å®Œæ•´æ€§ã€‚
    result = subprocess.run([
        'ffmpeg', '-y', '-err_detect', 'ignore_err',
        '-i', temp_raw,
        '-ar', '16000', '-ac', '1', '-c:a', 'libopus', '-b:a', '24k',
        '-movflags', 'faststart', final_name
    ], capture_output=True, text=True)

    if result.returncode != 0:
        print(f"âŒ [FFmpeg å ±éŒ¯] {result.stderr}")
        raise subprocess.CalledProcessError(result.returncode, result.args)
    
    if os.path.exists(temp_raw): os.remove(temp_raw)
    return os.path.getsize(final_name)

# --- [ä¸»æ¼”ç¿’ç¨‹åº] ---
def run_full_cycle_test():
    # 1. è£œçµ¦åˆå§‹åŒ–
    scra_key = os.environ.get("WEBSCRAP_API_KEY")
    sb_url, sb_key = os.environ.get("SUPABASE_URL"), os.environ.get("SUPABASE_KEY")
    r2_id, r2_secret = os.environ.get("R2_ACCESS_KEY_ID"), os.environ.get("R2_SECRET_ACCESS_KEY")
    r2_acc, r2_bucket = os.environ.get("R2_ACCOUNT_ID"), os.environ.get("R2_BUCKET_NAME")
    
    supabase: Client = create_client(sb_url, sb_key)
    ai_agent = AIAgent()
    s3_client = boto3.client('s3', endpoint_url=f'https://{r2_acc}.r2.cloudflarestorage.com',
                             aws_access_key_id=r2_id, aws_secret_access_key=r2_secret)

    # ğŸš€ 2. é ˜å– 1 ç­†å¾…å‘½ç‰©è³‡ (å»£åŸŸé›·é”ç‰ˆ)
    res = supabase.table("mission_queue").select("*") \
        .eq("status", "pending") \
        .not_.is_("audio_url", "null") \
        .or_("scrape_status.eq.success,scrape_status.eq.pending") \
        .order("created_at", desc=True).limit(1).execute()

    if not res.data: 
        print("â˜• [å¾…å‘½] æš«ç„¡ç‰©è³‡éœ€æ¼”ç¿’ã€‚")
        return
    
    m = res.data[0]
    # ğŸš€ 3. æœ¬åœ°å…ˆè¡Œè§£ææœ€çµ‚ç›´é€£ç¶²å€
    total_size, resolved_url = get_target_specs(m['audio_url'])
    total_mb = total_size / (1024 * 1024)
    
    if total_size == 0 or total_mb > 60:
        print(f"ğŸ›‘ [æ’¤é€€] ç‰©è³‡é«”ç© ({total_mb:.2f}MB) è¶…æ¨™ï¼Œä¸äºˆæ¬é‹ã€‚")
        return

    # ğŸš€ 4. åˆ†æ®µè¨ˆç®— (æ¡ç”¨æ‚¨é¸æ“‡çš„ 4.5MB ç©©å¥æ­¥èª¿)
    chunk_size = max(5.5 * 1024 * 1024, math.ceil(total_size / 20))
    num_chunks = math.ceil(total_size / chunk_size)
    if not os.path.exists('parts'): os.makedirs('parts')

    print(f"ğŸš€ [æ¼”ç¿’é–‹å§‹] {m['source_name']} | ç¸½é‡ï¼š{total_mb:.2f}MB | åˆ†æ®µï¼š{num_chunks}")

    # ğŸš€ 5. åºåˆ—åŒ–æ“¬æ…‹æ¬é‹
    for i in range(num_chunks):
        if i > 0: time.sleep(random.uniform(8.5, 16.2))
        start = i * chunk_size
        end = min(start + chunk_size - 1, total_size - 1)

        # ä¸€è¡Œè¨»è§£ï¼šä½¿ç”¨è§£æå¾Œçš„ resolved_url é¿é–‹è½‰å€é¢¨éšªã€‚
        chunk_data = fetch_chunk_via_proxy(resolved_url, start, end, scra_key)
        
        if chunk_data:
            with open(f"parts/part_{i}.bin", "wb") as f: f.write(chunk_data)
            print(f"âœ… ç‰‡æ®µ {i} æˆåŠŸã€‚")
        else:
            print(f"âŒ [æ–·ä¾›] ç‰‡æ®µ {i} é­æ‹’ã€‚")
            return

    # ğŸš€ 6. ç¸«åˆã€å£“ç¸®èˆ‡ AI åˆ†æ
    final_opus = f"RELAY_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{m['source_name']}.opus"
    c_size = assemble_and_compress(m['id'], num_chunks, final_opus, resolved_url)
    
    print(f"ğŸ§  [AI è¡Œå‹•] å‘¼å«æ™ºå›Šåœ˜åŸ·è¡Œæ‘˜è¦...")
    analysis, _, duration = ai_agent.generate_gold_analysis(final_opus)

    if analysis:
        # ğŸš€ 7. Telegram å ±æˆ°
        report_msg = ai_agent.format_mission_report("Relay-V3.4", m['episode_title'], resolved_url, analysis, datetime.now().strftime("%m/%d/%y"), duration, m['source_name'])
        report_msg += f"\n\nğŸ“Š [ç‰©æµæ•¸æ“š]\nåŸå§‹ï¼š{total_mb:.2f}MB\nå£“ç¸®å¾Œï¼š{c_size/(1024*1024):.2f}MB"
        requests.post(f"https://api.telegram.org/bot{os.environ.get('TELEGRAM_BOT_TOKEN')}/sendMessage", 
                      json={"chat_id": os.environ.get("TELEGRAM_CHAT_ID"), "text": report_msg, "parse_mode": "Markdown"})

    # ğŸš€ 8. å…¥åº«èˆ‡æ­¸æª”
    s3_client.upload_file(final_opus, r2_bucket, final_opus, ExtraArgs={'ContentType': 'audio/ogg'})
    supabase.table("mission_queue").update({"status": "completed", "r2_url": final_opus}).eq("id", m['id']).execute()
    print(f"ğŸ† [æ¼”ç¿’é”æˆ] ç‰©è³‡å·²å…¥åº«ä¸”ä»»å‹™å·²çµæ¡ˆã€‚")
    if os.path.exists(final_opus): os.remove(final_opus)

if __name__ == "__main__":
    run_full_cycle_test()