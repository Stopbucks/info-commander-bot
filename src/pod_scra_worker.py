# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/pod_scra_worker.py v5.7 (èªæ³•ä¿®æ­£ç‰ˆ)
# è·è²¬ï¼šé ˜å–ä»»å‹™ -> ä¸²æµä¸‹è¼‰ -> ç›´é€ R2 -> ç‹€æ…‹æ›´æ–°
# ---------------------------------------------------------
import os
import time
import requests
import boto3
from supabase import create_client, Client
from dotenv import load_dotenv

# ä¸€è¡Œè¨»è§£ï¼šå•Ÿå‹•ç’°å¢ƒè®Šæ•¸åŠ è¼‰ã€‚
load_dotenv()

def get_supabase_client():
    # ä¸€è¡Œè¨»è§£ï¼šç²å–ä¸¦å¼·åˆ¶ä¿®å‰ªè®Šæ•¸ç©ºç™½ä»¥é˜²èªè­‰éŒ¯èª¤ã€‚
    url = os.environ.get("SUPABASE_URL", "").strip()
    key = os.environ.get("SUPABASE_KEY", "").strip()
    
    if not url or not key:
        print("âŒ [éŒ¯èª¤] ç’°å¢ƒè®Šæ•¸è®€å–å¤±æ•—ï¼Œè«‹æª¢æŸ¥è¨­å®š")
        return None

    try:
        # ä¸€è¡Œè¨»è§£ï¼šå»ºç«‹èˆ‡ Supabase çš„èªè­‰é€£ç·šã€‚
        return create_client(url, key)
    except Exception as e:
        print(f"âŒ [é€£ç·šå ±éŒ¯] {str(e)}")
        raise e

def get_s3_client():
    # ä¸€è¡Œè¨»è§£ï¼šå»ºç«‹èˆ‡ R2 å€‰åº«çš„é€šè¨Šé€£æ¥ã€‚
    return boto3.client(
        's3',
        endpoint_url=os.environ.get("R2_ENDPOINT_URL"),
        aws_access_key_id=os.environ.get("R2_ACCESS_KEY_ID"),
        aws_secret_access_key=os.environ.get("R2_SECRET_ACCESS_KEY"),
        region_name="auto"
    )

def upload_to_r2(file_path, bucket_name, object_name):
    # ä¸€è¡Œè¨»è§£ï¼šå°‡æš«å­˜æª”æ¡ˆæ¨é€è‡³ R2 å€‰åº«ã€‚
    s3 = get_s3_client()
    try:
        s3.upload_file(file_path, bucket_name, object_name)
        print(f"âœ… [å…¥åº«æˆåŠŸ] æª”æ¡ˆå·²å­˜è‡³: {object_name}")
        return True
    except Exception as e:
        print(f"âŒ [å…¥åº«å¤±æ•—] éŒ¯èª¤åŸå› : {e}")
        return False

def run_logistics_mission():
    # ä¸€è¡Œè¨»è§£ï¼šå•Ÿå‹•è‡ªå‹•åŒ–ç‰©æµå·¡é‚ã€‚
    sb = get_supabase_client()
    
    if not sb:
        print("âŒ [ç‰©æµéƒ¨] é€šè¡Œè­‰æ ¡é©—å¤±æ•—ï¼Œä»»å‹™ä¸­æ­¢ã€‚")
        return

    while True:
        print(f"ğŸ•’ [å“¨å…µå·¡é‚] æ­£åœ¨æƒæä»»å‹™éšŠåˆ— (Target: pending)...")
        
        try:
            # ä¸€è¡Œè¨»è§£ï¼šæŠ“å–å¾…è™•ç†ä¸”åµå¯ŸæˆåŠŸçš„ä»»å‹™ã€‚
            mission = sb.table("mission_queue").select("*").eq("status", "pending").eq("scrape_status", "success").limit(1).execute()
            
            if mission.data:
                task = mission.data[0]
                task_id = task['id']
                audio_url = task['audio_url']
                file_name = f"{task['pub_date']}_{task['title'][:30]}.m4a"
                temp_path = f"/tmp/{file_name}"

                print(f"ğŸš› [èµ·é‹] åµæ¸¬åˆ°ç‰©è³‡: {task['title']}")

                # ä¸€è¡Œè¨»è§£ï¼šä½¿ç”¨ä¸²æµä¸‹è¼‰ä»¥ç¯€çœå…§å­˜ç©ºé–“ã€‚
                resp = requests.get(audio_url, timeout=60, stream=True)
                resp.raise_for_status()
                
                with open(temp_path, "wb") as f:
                    for chunk in resp.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                if upload_to_r2(temp_path, os.environ.get("R2_BUCKET_NAME"), file_name):
                    # ä¸€è¡Œè¨»è§£ï¼šå®Œæˆå¾ŒåŒæ­¥æ›´æ–°è³‡æ–™åº«ç‹€æ…‹ã€‚
                    sb.table("mission_queue").update({
                        "status": "stored_in_r2",
                        "r2_path": file_name
                    }).eq("id", task_id).execute()
                    print(f"ğŸ† [çµæ¡ˆ] ä»»å‹™ {task_id} æ¬é‹å®Œç•¢ã€‚")
                
                if os.path.exists(temp_path): os.remove(temp_path)

            else:
                print(f"â˜• [ç‰©æµéƒ¨] ç›®å‰ç„¡å¾…æ¬é‹ç‰©è³‡ï¼Œ5 åˆ†é˜å¾Œå†æ¬¡å·¡é‚ã€‚")
        
        except Exception as e:
            print(f"âš ï¸ [å·¡é‚æ³¢å‹•] ç•°å¸¸å›å ±: {e}")
            time.sleep(60)
            continue
        
        time.sleep(300)

if __name__ == "__main__":
    run_logistics_mission()