
# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/pod_scra_cleaner.py v1.1 (æ³•å®šæ¨™ç±¤æ ¡æº–ç‰ˆ)
# ä»»å‹™ï¼šå…©éšæ®µåº«å­˜ç®¡ç† R2 & Supabase (7å¤©é‡ç”Ÿ / 14å¤©æ¸…é™¤)
# ä¿®æ­£ï¼šç²¾æº–å°ä½ GitHub Secrets æ¨™ç±¤ R2_SECRET_ACCESS_KEY
# ---------------------------------------------------------
import os, boto3
from supabase import create_client, Client
from datetime import datetime, timedelta, timezone

def run_cleanup_plan():
    # 1. åˆå§‹åŒ–ç’°å¢ƒè®Šæ•¸ (æ ¹æ“šæ³•å®šæ¸…å–®æ ¡æº–)
    sb_url = os.environ.get("SUPABASE_URL")
    sb_key = os.environ.get("SUPABASE_KEY")
    
    r2_id = os.environ.get("R2_ACCESS_KEY_ID")
    # ğŸš€ ä¿®æ­£ï¼šå°ä½æ³•å®šæ¸…å–®ä¸­çš„åç¨±
    r2_secret = os.environ.get("R2_SECRET_ACCESS_KEY") 
    r2_account_id = os.environ.get("R2_ACCOUNT_ID")
    r2_bucket = os.environ.get("R2_BUCKET_NAME", "pod-scra-vault") # å„ªå…ˆä½¿ç”¨ Secret å®šç¾©

    # 2. å®‰å…¨æ€§æª¢æŸ¥ï¼šç¢ºä¿é—œéµæ­¦å™¨çš†æœ‰å½ˆè—¥
    if not all([sb_url, sb_key, r2_id, r2_secret, r2_account_id]):
        print("âŒ [æ¸…ç†å…µ] ç’°å¢ƒè®Šæ•¸å°ä½å¤±æ•—ï¼Œç‚ºé¿å…èª¤åˆªï¼Œä»»å‹™ä¸­æ­¢ã€‚")
        return

    # åˆå§‹åŒ–çµ„ä»¶
    supabase: Client = create_client(sb_url, sb_key)
    s3_client = boto3.client(
        's3', 
        endpoint_url=f'https://{r2_account_id}.r2.cloudflarestorage.com',
        aws_access_key_id=r2_id, 
        aws_secret_access_key=r2_secret,
        region_name='auto' # ğŸš€ å¢åŠ ï¼šæ˜ç¢ºæŒ‡å®š region æé«˜ boto3 ç©©å®šæ€§
    )

    now = datetime.now(timezone.utc)
    seven_days_ago = (now - timedelta(days=7)).isoformat()
    fourteen_days_ago = (now - timedelta(days=14)).isoformat()

    # --- éšæ®µä¸€ï¼š7å¤©é‡ç”Ÿè¨ˆç•« (Rebirth) ---
    print(f"ğŸ”„ [éšæ®µä¸€] æ­£åœ¨åŸ·è¡Œé‡ç”Ÿç¨‹åº (LT 7 Days)...")
    try:
        # å°‡è¶…é 7 å¤©ä¸”æœªå®Œæˆçš„ä»»å‹™é‡ç½®ç‚º pendingï¼Œç”± Scanner é‡æ–°å˜—è©¦
        rebirth_query = supabase.table("mission_queue").update({
            "scrape_status": "pending",
            "status": "pending",
            "mission_type": "rebirth_retry"
        }).lt("created_at", seven_days_ago).neq("status", "completed").execute()
        print(f"âœ… é‡ç”Ÿå®Œæˆï¼Œå…±è¨ˆ {len(rebirth_query.data)} ç­†ä»»å‹™é‡è¿”æˆ°å ´ã€‚")
    except Exception as e:
        print(f"âš ï¸ éšæ®µä¸€é‡ç”Ÿå¤±æ•—: {e}")

    # --- éšæ®µäºŒï¼š14å¤©å ±å»¢æ¸…ç† (Purge) ---
    print(f"ğŸ§¹ [éšæ®µäºŒ] æ­£åœ¨æ¸…ç† 14 å¤©å‰çš„é™³èˆŠæ•¸æ“šèˆ‡ R2 ç‰©è³‡...")
    try:
        old_missions = supabase.table("mission_queue").select("id, r2_url") \
            .lt("created_at", fourteen_days_ago).execute()

        if old_missions.data:
            for m in old_missions.data:
                # åˆªé™¤ R2 å¯¦é«”æª”æ¡ˆ (å¦‚æœå­˜åœ¨)
                if m.get('r2_url'):
                    try:
                        s3_client.delete_object(Bucket=r2_bucket, Key=m['r2_url'])
                        print(f"ğŸ—‘ï¸ å·²ç§»é™¤ R2 æ®˜éª¸: {m['r2_url']}")
                    except Exception as e:
                        print(f"âš ï¸ R2 ç‰©è³‡ç§»é™¤ç•°å¸¸ (å¯èƒ½å·²ä¸å­˜åœ¨): {e}")
                
                # åˆªé™¤ Supabase ç´€éŒ„
                supabase.table("mission_queue").delete().eq("id", m['id']).execute()
            
            print(f"âœ… å ±å»¢æ¸…ç†å®Œç•¢ï¼Œå…±ç§»é™¤äº† {len(old_missions.data)} ç­†æ­·å²ç´€éŒ„ã€‚")
        else:
            print("â˜• æˆ°å ´æ¸…ç†å®Œç•¢ï¼Œç›®å‰ç„¡éæœŸç‰©è³‡ã€‚")
    except Exception as e:
        print(f"âš ï¸ éšæ®µäºŒæ¸…ç†å¤±æ•—: {e}")

if __name__ == "__main__":
    run_cleanup_plan()