# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/pod_scra_scanner.py v1.1
# ä»»å‹™ï¼šçµ±ä¸€ç ´é˜²æƒæå™¨ã€‚æ”¯æ´ 5 å¤§æ¨¡å¼ï¼Œç¢ºä¿åƒæ•¸å°ä½ã€‚
# è¨»è¨˜ï¼šZenRows ç”³è«‹æ—¥ 2/18ï¼Œé è¨ˆ 3/3 æ£„ç”¨æª¢æŸ¥ã€‚
# ---------------------------------------------------------
# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/pod_scra_scanner.py v1.3 (ä¸»åŠ›æ›´è¿­ç‰ˆ)
# ä»»å‹™ï¼šHasdata ç«åŠ›å‡ç´šã€ScraperAPI å¤šè»Œå‚™æ´ã€WebScraping å®šä½ä¿®æ­£ã€‚
# ---------------------------------------------------------
import requests, urllib.parse

def fetch_html(provider_key, target_url, keys):
    # ä¸€è¡Œè¨»è§£ï¼šå…¨åŸŸè¶…æ™‚è¨­å®š 60 ç§’ï¼Œç¢ºä¿ Render æ“šé»èƒ½é †åˆ©ç°½æ”¶å¾Œé€²è¡Œé•·æ™‚é–“ç­‰å¾…ã€‚
    TO = 60 
    
    try:
        # 1. ä¸»åŠ›éƒ¨éšŠï¼šSCRAPERAPI (æ¯æœˆ 1,000 é»)
        if provider_key == "SCRAPERAPI":
            # ä¸€è¡Œè¨»è§£ï¼šé–‹å•Ÿ render=true ä»¥å¼·åˆ¶åŸ·è¡Œ JS è§£æï¼Œç©¿é€ Podbay çš„å‹•æ…‹ç¶²å€é˜²ç·šã€‚
            params = {'api_key': keys['SCRAPERAPI'], 'url': target_url, 'render': 'true'}
            return requests.get('https://api.scraperapi.com', params=params, timeout=TO)
            
        # 2. è‡¨çµ‚éƒ¨éšŠï¼šZENROWS (è©¦ç”¨æœŸå°‡å±†ï¼Œåƒ…ä¾›ç·Šæ€¥èª¿åº¦)
        elif provider_key == "ZENROWS":
            params = {'apikey': keys['ZENROWS'], 'url': target_url, 'js_render': 'true', 'premium_proxy': 'true'}
            return requests.get('https://api.zenrows.com/v1/', params=params, timeout=TO)

        # 3. è½‰é‹å°ˆå“¡ï¼šWEBSCRAPING (2,000 é»/æœˆï¼Œæ“…é•·è™•ç†è½‰å€èˆ‡éš±è—ç¶²å€)
        elif provider_key == "WEBSCRAPING":
            # ä¸€è¡Œè¨»è§£ï¼šåˆ©ç”¨å…¶ç©©å®šçš„ JS æ¸²æŸ“èƒ½åŠ›ï¼Œè² è²¬è¿½è¹¤ä¸¦è§£æ Podbay å…§éƒ¨çš„éš±è—éŸ³è¨Šæµã€‚
            params = {'api_key': keys['WEBSCRAP'], 'url': target_url, 'js': 'true', 'proxy': 'datacenter'}
            return requests.get('https://api.webscraping.ai/html', params=params, timeout=TO)
            
        # 4. å‚™æ´ç ´åŸæ§Œï¼šSCRAPEDO (1,000 é»/æœˆ)
        elif provider_key == "SCRAPEDO":
            encoded_url = urllib.parse.quote(target_url)
            api_url = f"https://api.scrape.do?token={keys['SCRAPEDO']}&url={encoded_url}&render=true"
            return requests.get(api_url, timeout=TO)

        # 5. ç‰¹ç¨®éƒ¨éšŠï¼šHASDATA (æ¯æ—¥ 100 é»ï¼Œä½å®…ä»£ç†ç«åŠ›åŠ å¼·ç‰ˆ)
        elif provider_key == "HASDATA":
            # ğŸ¯ æ ¸å¿ƒå‡ç´šï¼šå°‡ proxy_type ç”± datacenter æå‡è‡³ residentialã€‚
            # ä¸€è¡Œè¨»è§£ï¼šæ›è£ã€Œä½å®…ä»£ç†ã€éš±èº«æ–—ç¯·ï¼Œä»¥æœ€é«˜ç©¿é€åŠ›æ‡‰å° Podbay çš„æœ€çµ‚å°é–ã€‚
            headers = {'x-api-key': keys['HASDATA']}
            params = {
                'url': target_url,
                'js_render': 'true',
                'proxy_type': 'residential' # ğŸš€ æå‡ç‚ºä½å®…ä»£ç†
            }
            return requests.get('https://api.hasdata.com/scrape', headers=headers, params=params, timeout=TO)

        return None
    
    except Exception as e:
        print(f"âš ï¸ [Scanner ç•°å¸¸] {provider_key} åµå¯Ÿæ©Ÿå¤±è¯: {e}")
        return None