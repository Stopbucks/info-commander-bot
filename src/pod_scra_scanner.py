# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ï¼šsrc/pod_scra_scanner.py v1.1
# ä»»å‹™ï¼šçµ±ä¸€ç ´é˜²æƒæå™¨ã€‚æ”¯æ´ 5 å¤§æ¨¡å¼ï¼Œç¢ºä¿åƒæ•¸å°ä½ã€‚
# è¨»è¨˜ï¼šZenRows ç”³è«‹æ—¥ 2/18ï¼Œé è¨ˆ 3/3 æ£„ç”¨æª¢æŸ¥ã€‚
# ---------------------------------------------------------
import requests, urllib.parse

def fetch_html(provider_key, target_url, keys):
    """
    åŸ·è¡ŒæŠ“å–ä»»å‹™ã€‚
    provider_key: æ¨¡å¼ç°¡ç¨± (ä¾‹å¦‚ 'ZENROWS')
    keys: åŒ…å«æ‰€æœ‰ API Key çš„å­—å…¸
    """
    try:
        if provider_key == "SCRAPERAPI":
            params = {'api_key': keys['SCRAPERAPI'], 'url': target_url, 'render': 'true'}
            return requests.get('https://api.scraperapi.com', params=params, timeout=60)
            
        elif provider_key == "ZENROWS":
            # ğŸ’¡ æˆ°å ±ï¼šç›®å‰ä¸»æˆ°åŠ›ï¼Œè©¦ç”¨è‡³ 3/3ã€‚
            params = {'api_key': keys['ZENROWS'], 'url': target_url, 'js_render': 'true'}
            return requests.get('https://api.zenrows.com/v1/', params=params, timeout=60)
            
        elif provider_key == "WEBSCRAPING":
            # ğŸ’¡ æˆ°å ±ï¼šæ—¥å¾Œæ¨¡å¼äºŒå„ªå…ˆï¼Œæ¯æœˆ 2,000 é»
            params = {'api_key': keys['WEBSCRAP'], 'url': target_url, 'js': 'true', 'proxy': 'datacenter'}
            return requests.get('https://api.webscraping.ai/html', params=params, timeout=60)
            
        elif provider_key == "SCRAPEDO":
            # ğŸ’¡ æˆ°å ±ï¼šå‚™æ´ç ´åŸæ§Œï¼Œæ¯æœˆ 1,000 é»
            encoded_url = urllib.parse.quote(target_url)
            api_url = f"https://api.scrape.do?token={keys['SCRAPEDO']}&url={encoded_url}&render=true"
            return requests.get(api_url, timeout=60)

            #---  å¢åŠ  HasData æ¡ç”¨ Header å¸¶å…¥ Keyï¼Œæ¯æ¬¡æˆåŠŸæŠ“å–éœ€ 10 é»ã€‚ ---#
        elif provider_key == "HASDATA":
            # ğŸ’¡ æˆ°å ±ï¼šå‚™æ´ç ´åŸæ§Œï¼Œæ¯æ—¥ 100 é»
            headers = {'x-api-key': keys['HASDATA']}
            params = {
                'url': target_url,
                'js_render': 'true',      # å¿…é–‹ï¼Œä»¥æ‡‰å° Podbay
                'proxy_type': 'datacenter' # ä½¿ç”¨ DC ä»£ç†ä»¥ç¯€çœé»æ•¸
            }
            return requests.get('https://api.hasdata.com/scrape', headers=headers, params=params, timeout=60)
# -----(å®šä½ç·š)ä»¥ä¸Šä¿®æ”¹----
        return None
    except Exception as e:
        print(f"âš ï¸ [Scanner ç•°å¸¸] {provider_key} é€£ç·šå¤±æ•—: {e}")
        return None