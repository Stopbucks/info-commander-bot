# =========================================================
# RS (Rescue-Standalone) ç¨ç«‹æ”»å …è…³æœ¬ - v1.0
# è·è²¬ï¼šä¸ä¾è³´ä»»ä½•å¤–éƒ¨ utilsï¼Œç›´æ¥å°æ¥ ScraperAPI åŸ·è¡Œå¯¦æˆ°ä¸‹è¼‰ã€‚
# =========================================================
import os
import requests

def run_simple_rs():
    # 1. é ˜å–è£å‚™ (API KEY)
    api_key = os.environ.get('SCRAP_API_KEY', '').strip()
    if not api_key:
        print("âŒ [RS] æ‰¾ä¸åˆ° API KEYï¼Œè«‹æª¢æŸ¥ GitHub Secretsã€‚")
        return

    # 2. å°è£æœ€æ¨™æº–çš„ä»£ç†åœ°å€
    proxy_url = f"http://scraperapi:{api_key}@proxy-server.scraperapi.com:8001"
    proxies = {"http": proxy_url, "https": proxy_url}

    # ğŸ¯ å…ˆæ¸¬è©¦ Google (æ¥µç°¡ç›®æ¨™)ï¼Œå†æ¸¬è©¦ Archive (å¯¦æˆ°ç›®æ¨™)
    test_url = "http://www.google.com" 
    
    print(f"ğŸ“¡ [RS ä½ç©ºåµå¯Ÿ] æ­£åœ¨å˜—è©¦é€é 8001 ç«¯å£é€£ç·šè‡³: {test_url}")

    try:
        # ğŸ’¡ æˆ°è¡“æ ¸å¿ƒï¼šä¸è‡ªå®šç¾©ä»»ä½•æ¨™é ­ï¼Œè®“æ¨™æº– requests è™•ç†æ‰€æœ‰å¿…è¦æ¬„ä½
        # ğŸ’¡ ä½¿ç”¨ http (é s) æ¸¬è©¦ï¼Œé€²ä¸€æ­¥é™ä½æ¡æ‰‹å¤±æ•—é¢¨éšª
        resp = requests.get(test_url, proxies=proxies, timeout=30, verify=False)
        
        print(f"ğŸš© [åµå¯Ÿå›å ±] ç‹€æ…‹ç¢¼: {resp.status_code}")
        if resp.status_code == 200:
            print("âœ… [é¦–æˆ°å¤§æ·] ä»£ç†é€šé“å®Œå…¨æš¢é€šï¼å…è²»ç‰ˆæ”¯æ´æ­¤è·¯å¾‘ã€‚")
        else:
            print(f"âš ï¸ [é€£ç·šæˆåŠŸä½†è¢«æ“‹] ä¼ºæœå™¨å›å‚³: {resp.text[:100]}")

    except Exception as e:
        print(f"ğŸ’¥ [åµå¯Ÿå´©æ½°] åŸå› : {e}")

if __name__ == "__main__":
    run_simple_rs()