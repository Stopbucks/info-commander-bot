# ---------------------------------------------------------
# æœ¬ç¨‹å¼ç¢¼ç‚ºï¼šPodcast_ai_agentï¼Œè² è²¬æç¤ºè©çµ¦äºˆgemini-2.5(æ¨¡å‹å‹¿å‹•)åˆ¤æ–·+å ±å‘Š
# ---------------------------------------------------------
import google.generativeai as genai
import os
import re
import time
from podcast_prompts import GEMINI_MAIN_PROMPT, WEEKLY_STRATEGIC_PROMPT, SIMPLE_FALLBACK_PROMPT
from groq import Groq

class AIAgent:
    """
    ğŸ§  [æ™ºå›Šåœ˜] è·è²¬ï¼šåŸ·è¡Œæ ¸å¿ƒ Promptã€ç”Ÿæˆé«˜å“è³ªæƒ…å ±ã€ç¶­æŒæ ¼å¼ä¸€è‡´æ€§ã€‚
    """
    def __init__(self):
        self.api_key = os.environ.get("GEMINI_API_KEY")
        if self.api_key:
            genai.configure(api_key=self.api_key)
        # ğŸš€ å‡ç´šç‚ºäºŒä»£å¤§è…¦ 2.5 ç‰ˆæœ¬
        self.model = genai.GenerativeModel("gemini-2.5-flash")
# --- [æ›´æ–°è™•ï¼šæ–°å¢ Groq é…ç½®] ---
        self.groq_key = os.environ.get("GROQ_API_KEY")
        self.groq_client = Groq(api_key=self.groq_key) if self.groq_key else None

    # ---------------------------------------------------------
    # âš”ï¸ æ¸¸æ“ŠéšŠå°ˆç”¨ï¼šGroq + Opus æ¥µé€Ÿæ‘˜è¦æµç¨‹ [cite: 2026-01-16]
    # ---------------------------------------------------------
    def generate_groq_summary(self, opus_file_path):
        """ğŸš€ [g-å°éšŠ] ä½¿ç”¨ Groq åŸ·è¡Œè½‰å¯«èˆ‡æ‘˜è¦ï¼Œå¾¹åº•é¿é–‹ GCP æµé‡ [cite: 2026-01-16]"""
        if not self.groq_client:
            print("âŒ [Groq æ•…éšœ] æœªåµæ¸¬åˆ° GROQ_API_KEYã€‚")
            return None

        try:
            print(f"ğŸ§¬ [Groq å•Ÿå‹•] æ­£åœ¨è§£æ Opus éŸ³æª”ï¼š{os.path.basename(opus_file_path)}")
            
            # Step 1: èªéŸ³è½‰æ–‡å­— (ä½¿ç”¨ Whisper-large-v3 æ¨¡å‹) [cite: 2026-01-16]
            with open(opus_file_path, "rb") as file:
                transcription = self.groq_client.audio.transcriptions.create(
                    file=(opus_file_path, file.read()),
                    model="whisper-large-v3",
                    response_format="text",
                    language="en"  # å¼·åˆ¶è‹±æ–‡è­˜åˆ¥ä»¥æé«˜æ¼”è¬›æº–ç¢ºåº¦
                )

            # Step 2: å‘¼å«"llama-3.1-70b-versatile",æ‘˜è¦åˆ†æï¼Œå¼•ç”¨ SIMPLE_FALLBACK_PROMPT 
            print(f"ğŸ“ [æ‘˜è¦ä¸­] æ­£åœ¨ç™¼èµ· Groq è¼•é‡åŒ–ç­–å±•...")
            completion = self.groq_client.chat.completions.create(
                
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": SIMPLE_FALLBACK_PROMPT},
                    {"role": "user", "content": f"è«‹åˆ†æä»¥ä¸‹ Podcast é€å­—ç¨¿å…§å®¹ï¼š\n\n{transcription}"}
                ],
                temperature=0.5,
                max_tokens=1024
            )
            
            return completion.choices[0].message.content

        except Exception as e:
            print(f"âŒ [Groq å´©æ½°] åŸ·è¡Œç•°å¸¸ï¼š{str(e)}")
            return f"âš ï¸ æ‘˜è¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ Groq é¡åº¦æˆ–é€£ç·šç‹€æ³ã€‚éŒ¯èª¤ç´°ç¯€ï¼š{str(e)}"
        

    def generate_gold_analysis(self, audio_file_path):
        """åŸ·è¡Œæ·±åº¦ AI åˆ†æï¼Œå…·å‚™è¶…æ™‚é˜²ç¦¦ã€å¼·åˆ¶æ¸…ç†èˆ‡æˆ°è¡“è¿½è¹¤åˆ—å° [cite: 2026-02-01]"""
        start_time = time.time()
        uploaded_file = None
        
        print(f"ğŸ§  [AIä»»å‹™] é–‹å§‹åˆ†æç›®æ¨™éŸ³æª”ï¼š{os.path.basename(audio_file_path)}")
        
        try:
            # 1. è¡›æ˜Ÿä¸Šå‚³
            print(f"ğŸ›°ï¸ [1/4] æ­£åœ¨å°‡éŸ³æª”æŠ•é€è‡³ Google è‡¨æ™‚ç©ºé–“...")
            uploaded_file = genai.upload_file(audio_file_path, mime_type="audio/mpeg")
            
            # 2. ç‹€æ…‹ç›£æ§
            retries = 30
            print(f"â³ [2/4] ç­‰å¾…é›²ç«¯è½‰ç¢¼èˆ‡ç’°å¢ƒå°±ç·’ (é è¨ˆ 30-150 ç§’)...")
            while uploaded_file.state.name == "PROCESSING" and retries > 0:
                if retries % 6 == 0:  # æ¯éš” 30 ç§’å°ä¸€æ¬¡ç‹€æ…‹ï¼Œé¿å…æ´—ç‰ˆ
                    print(f"   ... è¡›æ˜Ÿå›å ±ï¼šè™•ç†ä¸­ (å‰©é¤˜å˜—è©¦æ¬¡æ•¸: {retries})")
                time.sleep(5)
                uploaded_file = genai.get_file(uploaded_file.name)
                retries -= 1
            
            if retries <= 0:
                print("âŒ [2/4 æ•…éšœ] è¡›æ˜Ÿè¶…æ™‚ï¼Google ä¼ºæœå™¨è™•ç†éä¹…ï¼Œå•Ÿå‹•é˜²ç¦¦æ€§ç†”æ–·ã€‚")
                return None, 0, 0
            
            print(f"âœ… [2/4] ç’°å¢ƒå°±ç·’ï¼ŒéŸ³æª”å·²è§£é–ã€‚")

            # 3. æ ¸å¿ƒæ¨ç†
            print(f"ğŸ§¬ [3/4] æ™ºå›Šåœ˜å•Ÿå‹•ï¼šæ­£åœ¨ç™¼èµ· Gemini æ·±åº¦ç­–å±•åˆ†æ...")
            response = self.model.generate_content([GEMINI_MAIN_PROMPT, uploaded_file])
            final_text = response.text
            
            # 4. æ•¸æ“šæå–
            score_match = re.search(r"ç¶œåˆæƒ…å ±åˆ†.*?(\d+)", final_text)
            q_score = int(score_match.group(1)) if score_match else 20
            
            duration_mins = max(1, round((time.time() - start_time) / 60))
            print(f"ğŸ† [4/4] åˆ†ææˆåŠŸï¼æƒ…å ±è©•åˆ†ï¼š{q_score} | ç¸½è€—æ™‚ï¼š{duration_mins} åˆ†é˜")
            
            return final_text, q_score, duration_mins
            
        except Exception as e:
            print(f"âŒ [AIå´©æ½°] åŸ·è¡ŒæœŸé–“é­é‡æ””æˆªæˆ–ç•°å¸¸ï¼š\n   â””â”€ éŒ¯èª¤ç´°ç¯€: {str(e)}")
            return None, 0, 0
            
        finally:
            # 5. å¼·åˆ¶è³‡æºå›æ”¶
            if uploaded_file:
                try:
                    uploaded_file.delete()
                    print("ğŸ§¹ [æ¸…ç†] é›²ç«¯è‡¨æ™‚è³‡æºå·²å®‰å…¨å›æ”¶ã€‚")
                except Exception as cleanup_err:
                    print(f"âš ï¸ [è­¦å‘Š] è³‡æºé‡‹æ”¾å—é˜»ï¼š{cleanup_err}")
    
    # ==========================================================================
    # --- ğŸ“‹ æˆ°ç•¥å ±å‘Šç³»çµ± (å–®ç¯‡å ±å‘Šã€é€±æˆ°ç•¥å ±ã€æœˆåº¦æƒ…å ±å ±) ---
    # ==========================================================================

    def format_mission_report(self, tier, title, link, content, date_label, duration, podcast_name, audio_duration="æœªçŸ¥"):
        """ğŸš€ [æ ¼å¼åŒ–] æ›´æ–°ä»¥æ”¯æ´æ¸¸æ“ŠéšŠæ¨™ç±¤èˆ‡ Opus èªªæ˜ [cite: 2026-01-16]"""
        
        # ğŸš€ ä¿®æ”¹è™•ï¼šæ–°å¢ Guerrilla æ¨™é ­
        headers = {
            "Gold": "ğŸ† é»ƒé‡‘ç­‰ç´š-æ·±åº¦ç­–å±•æƒ…å ±", 
            "Platinum": "ğŸ’¿ ç™½é‡‘ç­‰ç´š-ç¯€ç›®ç°¡è¨Š",
            "Guerrilla": "ğŸ“¡ g-å°éšŠ-æ¸¸æ“Šæƒ…å ±" 
        }
        header = f"{headers.get(tier, 'â„¹ï¸ æƒ…å ±é€šçŸ¥')} ({date_label})"

        # ğŸš€ ä¿®æ”¹è™•ï¼šæ›´æ–°æ±ºç­–ç­†è¨˜ï¼ŒåŠ å…¥æ¸¸æ“Šæˆ°æ¨¡å¼èªªæ˜ [cite: 2026-01-16]
        DECISION_NOTE = (
            "--- ğŸ“Š Info Commander æ±ºç­–ç­†è¨˜ ---\n"
            "ğŸ“¡ æ¸¸æ“Šæˆ°æ¨¡å¼ï¼šæ¡ç”¨ Opus å£“ç¸®èˆ‡ Groq æ‘˜è¦ï¼Œç¯€çœ 90% å‚³è¼¸æˆæœ¬ã€‚\n"
            "ğŸ’ æ¨¡å¼ç¿»è½‰ï¼šéŸ³æª” > 15MB å•Ÿå‹•ä¿åº•æŠ½æª¢ã€‚"
        )

        return (
            f"{header}\n\nğŸ™ï¸ é »é“ï¼š{podcast_name}\nğŸ“Œ æ¨™é¡Œï¼š{title}\n\n{content}\n\n"
            f"ğŸ”— æ”¶è½ï¼š{link}\nâ³ é•·åº¦ï¼š{audio_duration}\n"
            f"> AI è™•ç†è€—æ™‚ {duration} åˆ†é˜\n\n{DECISION_NOTE}"
        )

    def generate_weekly_strategic_report(self, monitor_summary):
        """ğŸ§  [æˆ°ç•¥å‡ç´š] å‘¼å« Gemini é€²è¡Œä¸€é€±æ·±åº¦é™¤éŒ¯èˆ‡æ¥­ç•Œæ ¡æº–åˆ†æ [cite: 2026-02-03]"""
        # ğŸš€ å¼•å…¥ç§»è‡³ prompts.py çš„é€±å ±å°ˆå±¬æç¤ºè©
        from podcast_prompts import WEEKLY_STRATEGIC_PROMPT
        
        try:
            # ğŸ’¡ ç¬¬ä¸€æ®µï¼šåŠ å…¥æˆ°ç•¥æŒ‡æ®éƒ¨æ¨™é ­
            header = "ğŸ›¡ï¸ **Info Commander é€±æˆ°ç•¥æˆ°å ±**\n"
            header += "ğŸ“ æœ¬é€±æˆ°å ´æ•¸æ“šèˆ‡è³ªåŒ–æ·±åº¦å–è­‰å¦‚ä¸‹ï¼š\n"
            header += "--------------------------------\n"
            
            # ğŸ’¡ ç¬¬äºŒæ®µï¼šèª¿ç”¨ Gemini åŸ·è¡Œé™¤éŒ¯éˆé­‚çš„æ·±åº¦åˆ†æ
            response = self.model.generate_content([WEEKLY_STRATEGIC_PROMPT, monitor_summary])
            
            return f"{header}\n{response.text}\n\nğŸ’¡ æŒ‡ä»¤ï¼šè‹¥åµå¯ŸæˆåŠŸç‡ä½æ–¼ 80%ï¼Œå»ºè­°æª¢æŸ¥ä»£ç† IP ä¿¡ç”¨åˆ†ã€‚"
        except Exception as e:
            return f"âŒ [å¤§è…¦éè¼‰] ç„¡æ³•ç”Ÿæˆé€±å ±ï¼ŒéŒ¯èª¤ç´°ç¯€: {str(e)}"

    def generate_monthly_strategic_report(self, consolidated_data):
        """ğŸ§  [æˆ°ç•¥åˆ†æ] è³ªé‡ä¸¦é‡ï¼šé‡å°å››é€±æ•¸æ“šé€²è¡Œè¶¨å‹¢åˆ†æèˆ‡æ”¹å–„å„ªåŒ– [cite: 2026-02-03]"""
        # ğŸš€ æ­¤è™•é ç•™çµ¦æœªä¾† V7.1 çš„ MONTHLY_STRATEGIC_PROMPT
        prompt = f"""
        ä½ ç¾åœ¨æ˜¯ Info Commander å°éšŠçš„é«˜ç´šæˆ°ç•¥åˆ†æå®˜ã€‚
        è«‹æ ¹æ“šéå»å››é€±çš„æ¡é›†æ•¸æ“šé€²è¡Œã€Œè³ªã€é‡ä¸¦é‡ã€çš„æœˆåº¦ç¸½çµã€‚
        
        ã€åˆ†æè¦æ±‚ã€‘ï¼š
        ğŸš« åš´ç¦ Markdown è¡¨æ ¼ã€‚âœ… ä½¿ç”¨æ¢åˆ—å¼åˆ†æã€‚
        ğŸ•µï¸ æ·±åº¦æ¢è¨ 403 æ””æˆªåŸå› èˆ‡ç°è‰²ä½œæˆ°ï¼ˆé ç†±ï¼‰çš„å…·é«”æˆæ•ˆã€‚
        
        ã€åŸå§‹æ•¸æ“šåŒ…ã€‘ï¼š
        {consolidated_data}
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"âŒ [å¤§è…¦éè¼‰] ç„¡æ³•ç”Ÿæˆæœˆå ±: {str(e)}"