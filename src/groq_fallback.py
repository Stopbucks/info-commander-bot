import os
import time
from groq import Groq

# ==========================================
# ğŸ”‘ 1. åˆå§‹åŒ–å€å¡Š
# ==========================================
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None

# ==========================================
# ğŸš€ 2. æ ¸å¿ƒå‚™æ´å‡½å¼ (æ™ºæ…§åˆ†æ®µåŠ å¼·ç‰ˆ)
# ==========================================
def run_fallback(file_path, system_prompt):
    """
    åŸ·è¡Œå‚™æ´è·¯å¾‘ï¼šéŸ³æª”è½‰éŒ„ -> æ™ºæ…§åˆ†æ®µåˆ†æ -> åˆä½µç”¢å‡ºã€‚
    è§£æ±º Groq å…è²»ç‰ˆ 6,000 TPM é™åˆ¶ï¼Œä¸¦é€é 100 ç§’å†·å»ç¢ºä¿ç©©å®šæ€§ã€‚
    """
    if not client:
        print("âŒ [groq_fallback] éŒ¯èª¤ï¼šæœªè¨­å®š GROQ_API_KEYã€‚")
        return None

    try:
        # --- Step 1: ğŸ™ï¸ åŸ·è¡Œ Whisper è½‰éŒ„ ---
        print("âš¡ [groq_fallback] å•Ÿå‹• Whisper è½‰éŒ„...")
        with open(file_path, "rb") as file:
            transcription = client.audio.transcriptions.create(
                file=(file_path, file.read()),
                model="whisper-large-v3",
                response_format="text",
                language="en"
            )
        
        # --- Step 2: ğŸ§  æ™ºæ…§åˆ†æ®µåˆ†æèˆ‡å¼·åŒ–å†·å» ---
        # èªªæ˜ï¼šé‡å° 6,000 TPM é™åˆ¶ï¼Œæ¯æ®µåˆ‡å‰²ç‚º 7000 å­—å…ƒ (ç´„ 4500 tokens)
        print(f"ğŸ“ è½‰éŒ„å®Œæˆ ({len(transcription)} å­—)ï¼Œå•Ÿå‹•åˆ†æ®µå†·å»åˆ†æ...")
        
        chunk_size = 7000 
        chunks = [transcription[i:i + chunk_size] for i in range(0, len(transcription), chunk_size)]
        partial_results = []

        for index, chunk in enumerate(chunks):
            part_no = index + 1
            print(f"â³ æ­£åœ¨åˆ†æç¬¬ {part_no}/{len(chunks)} æ®µ...")

            # ğŸš€ æ¤å…¥å¼•å° Promptï¼šå‘ŠçŸ¥ AI é€™æ˜¯åˆ‡å‰²æª”æ¡ˆä¸”éœ€è¦–ç‚ºä¸€é«”
            chunk_prompt = (
                f"ã€æ³¨æ„ï¼šé€™æ˜¯é•·é€å­—ç¨¿çš„ç¬¬ {part_no} éƒ¨åˆ†ï¼Œè«‹è¦–ç‚ºåŒä¸€å€‹æª”æ¡ˆè™•ç†ã€‚ã€‘\n\n"
                f"è«‹ä¾æ“šå…ˆå‰æŒ‡ç¤ºçš„æ ¼å¼é€²è¡Œåˆ†æï¼š\n\n{chunk}"
            )

            completion = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": chunk_prompt}
                ],
                temperature=0.5,
                max_tokens=2048
            )
            
            partial_results.append(completion.choices[0].message.content)

            # ğŸ’¤ ğŸš€ [å¼·åŒ–å†·å»] è™•ç†å®Œä¸€æ®µå¾Œè‹¥é‚„æœ‰ä¸‹ä¸€æ®µï¼Œå¼·åˆ¶ä¼‘æ¯ 100 ç§’ä»¥åˆ·æ–° TPM é…é¡
            if part_no < len(chunks):
                print(f"ğŸ’¤ ç‚ºäº†è¦é¿ TPM é™åˆ¶ï¼Œå¼·åˆ¶å†·å» 100 ç§’ä»¥æº–å‚™è™•ç†ä¸‹ä¸€æ®µ...")
                time.sleep(100)

        # åˆä½µæ‰€æœ‰æ®µè½çš„åˆ†ææˆæœ
        final_report = "\n\n=== (ä¸‹çºŒåˆ†æ®µæƒ…å ±) ===\n\n".join(partial_results)
        return final_report

    except Exception as e:
        print(f"âš ï¸ [groq_fallback] åŸ·è¡Œå¤±æ•—: {str(e)}")
        return None